{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script transform raw imagery (in not_a_dam_images and dam_images folders) to Darknet-formatted inputs for YOLOv3 (link to implementation Maanas?)\n",
    "\n",
    "Outputs in 2 directories:\n",
    "* images: has filename.png\n",
    "* labels: filename.txt : class, center x, center y, width, height (x y w h are all normalized (between 0 and 1) relative to image dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for South Af\n",
    "from osgeo import gdal\n",
    "import ogr\n",
    "import shapely.wkb\n",
    "import shapely.prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import image as mpimg\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "import logging\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "LOGGER.info(\"Logger in INFO mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "\n",
    "REQUEST_TIMEOUT = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "\n",
    "rel_path_to_charlie_root = \"../../..\"\n",
    "    \n",
    "#OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/YOLOready_imagery_6-7_made_6-21\")\n",
    "OUTPUTS_DIR = os.path.join(rel_path_to_charlie_root,\"data/Darknet-inputs_imagery_6-7_made_6-21\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_A_DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/not_a_dam_images\")\n",
    "DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/dam_images\")\n",
    "\n",
    "TM_WORLD_BORDERS_URL = 'https://storage.googleapis.com/ecoshard-root/ipbes/TM_WORLD_BORDERS_SIMPL-0.3_md5_15057f7b17752048f9bd2e2e607fe99c.zip'\n",
    "\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "if not os.path.exists(DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % DAM_IMAGE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just one of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsets of data inputs - for faster development purposes\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('5140_clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('362_not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full dataset \n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30337"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do run this one to merge dam_list and not_a_dam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36236"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_file_list = dam_file_list+not_a_dam_file_list\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(all_images_file_list)\n",
    "\n",
    "len(all_images_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set_portion = .2\n",
    "validation_set_portion = .15\n",
    "Dams_per_round = 1000 # = max_dams_per_record "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get South Africa geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url_to_file(url, target_file_path):\n",
    "    \"\"\"Use requests to download a file.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): url to file.\n",
    "        target_file_path (string): local path to download the file.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(target_file_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "        with open(target_file_path, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response.raw, target_file)\n",
    "        del response\n",
    "    except:\n",
    "        LOGGER.exception('download of {url} to {target_file_path} failed')\n",
    "        # mods from LOGGER.exception(f'download of {url} to {target_file_path} failed')\n",
    "        raise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "tm_world_borders_zip_path = os.path.join(\n",
    "        WORKSPACE_DIR, 'world_borders', os.path.basename(TM_WORLD_BORDERS_URL))\n",
    "if not os.path.exists(tm_world_borders_zip_path):\n",
    "    download_url_to_file(TM_WORLD_BORDERS_URL, tm_world_borders_zip_path)\n",
    "    with zipfile.ZipFile(tm_world_borders_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(WORKSPACE_DIR,'world_borders'))\n",
    "\n",
    "tm_world_borders_vector_path = os.path.join(\n",
    "    WORKSPACE_DIR,'world_borders', 'TM_WORLD_BORDERS-0.3.shp')\n",
    "\n",
    "tm_world_borders_vector = ogr.Open(tm_world_borders_vector_path)\n",
    "tm_world_borders_layer = tm_world_borders_vector.GetLayer()\n",
    "for border_feature in tm_world_borders_layer:\n",
    "    if border_feature.GetField('NAME') == 'South Africa':\n",
    "        sa_geom = border_feature.GetGeometryRef()\n",
    "        sa_geom_prep = shapely.prepared.prep(\n",
    "            shapely.wkb.loads(sa_geom.ExportToWkb()))\n",
    "        break\n",
    "LOGGER.debug(sa_geom_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Make YOLO-ready data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: Function to make YOLO_ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Darknet_inputs(images_file_list, iteration): \n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Takes in folder of dam pngs, folder of bounding box json files\n",
    "    Normalizes json data to darknet format (center x, center y, bbox width, bbox height)\n",
    "    Creates new directories in darknet format\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting to make Darknet formated data, round %d' % iteration)\n",
    "    \n",
    "    random.seed(iteration)\n",
    "    random_numbers_list = [random.random() for x in range(0, len(images_file_list))]\n",
    "    random_number_iterator = 0\n",
    "    \n",
    "    for image_path in images_file_list:\n",
    "        \n",
    "        # read in image\n",
    "        img = mpimg.imread(image_path)\n",
    "\n",
    "        # get width and height\n",
    "        img_w = img.shape[0]\n",
    "        img_h = img.shape[1]\n",
    "\n",
    "        # get matching bounding box json file\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if not os.path.exists(json_path):\n",
    "            raise NameError(\"can't find bbox for %s\" % json_path)\n",
    "            \n",
    "        # read json bounding box coordinates\n",
    "        with open(json_path, 'r') as json_file: \n",
    "            image_metadata = json.load(json_file)\n",
    "        \n",
    "        # normalize to x-center, y-center, width, and height of bbox\n",
    "        coords = image_metadata['pixel_bounding_box']\n",
    "        avg_x = (coords[2] + coords[0]) / (2 * img_w)\n",
    "        avg_y = (coords[1] + coords[3]) / (2 * img_h)\n",
    "        nrm_w = (coords[2] - coords[0]) / img_w\n",
    "        nrm_h = (coords[1] - coords[3]) / img_h\n",
    "        nrm_xywh = np.array([avg_x, avg_y, nrm_w, nrm_h])\n",
    "\n",
    "        # Define new label in YOLO format\n",
    "        if 'not_a_dam' in image_path:\n",
    "            dam_type = 'not_a_dam'\n",
    "            label_str = ''\n",
    "        else:\n",
    "            dam_type = 'dam'\n",
    "            label_str = '0 ' + str('%.6f'%nrm_xywh[0]) + ' ' + str('%.6f'%nrm_xywh[1]) + ' ' + str('%.6f'%nrm_xywh[2]) + ' ' + str('%.6f'%nrm_xywh[3])\n",
    "\n",
    "            \n",
    "            \n",
    "        # - - -   - - -   - - -   \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "            \n",
    "            \n",
    "        random_number = random_numbers_list[random_number_iterator]\n",
    "        random_number_iterator+=1\n",
    "        \n",
    "        if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # both for dams & not_a_dams\n",
    "            writer = 'southaf_set'\n",
    "            log = southaf_log\n",
    "        elif random_number < holdout_set_portion:\n",
    "            writer = 'test_set'\n",
    "            log = test_log\n",
    "        elif random_number > (1-validation_set_portion):\n",
    "            writer = 'validation_set'\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = 'training_set'\n",
    "            log = training_log\n",
    "            \n",
    "        # Write the file in the corresponding set\n",
    "        \n",
    "        ## Write image here:\n",
    "        filename = image_path.split(\"images/\")[1].replace('.png','')\n",
    "            \n",
    "        newimg_filepath = os.path.join(OUTPUTS_DIR,writer,'images',filename+'.png')\n",
    "        shutil.copyfile(image_path, newimg_filepath)\n",
    "        \n",
    "        \n",
    "        newtext_filepath = os.path.join(OUTPUTS_DIR,writer,'labels',filename+'.txt')\n",
    "        file = open(newtext_filepath, 'w')\n",
    "        file.write(label_str)\n",
    "        file.close()\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "        \n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make Darknet formated data, round 0\n",
      "training_log {'not_a_dam': 107, 'dam': 536}\n",
      "validation_log {'not_a_dam': 25, 'dam': 120}\n",
      "test_log {'not_a_dam': 35, 'dam': 162}\n",
      "southaf_log {'not_a_dam': 2, 'dam': 13}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 1\n",
      "training_log {'not_a_dam': 223, 'dam': 1068}\n",
      "validation_log {'not_a_dam': 55, 'dam': 251}\n",
      "test_log {'not_a_dam': 61, 'dam': 316}\n",
      "southaf_log {'not_a_dam': 6, 'dam': 20}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 2\n",
      "training_log {'not_a_dam': 321, 'dam': 1616}\n",
      "validation_log {'not_a_dam': 75, 'dam': 378}\n",
      "test_log {'not_a_dam': 84, 'dam': 486}\n",
      "southaf_log {'not_a_dam': 10, 'dam': 30}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 3\n",
      "training_log {'not_a_dam': 415, 'dam': 2150}\n",
      "validation_log {'not_a_dam': 96, 'dam': 505}\n",
      "test_log {'not_a_dam': 123, 'dam': 656}\n",
      "southaf_log {'not_a_dam': 15, 'dam': 40}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 4\n",
      "training_log {'not_a_dam': 508, 'dam': 2660}\n",
      "validation_log {'not_a_dam': 124, 'dam': 652}\n",
      "test_log {'not_a_dam': 156, 'dam': 834}\n",
      "southaf_log {'not_a_dam': 19, 'dam': 47}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 5\n",
      "training_log {'not_a_dam': 605, 'dam': 3190}\n",
      "validation_log {'not_a_dam': 151, 'dam': 767}\n",
      "test_log {'not_a_dam': 191, 'dam': 1009}\n",
      "southaf_log {'not_a_dam': 25, 'dam': 62}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 6\n",
      "training_log {'not_a_dam': 700, 'dam': 3703}\n",
      "validation_log {'not_a_dam': 176, 'dam': 890}\n",
      "test_log {'not_a_dam': 219, 'dam': 1203}\n",
      "southaf_log {'not_a_dam': 26, 'dam': 83}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 7\n",
      "training_log {'not_a_dam': 808, 'dam': 4220}\n",
      "validation_log {'not_a_dam': 200, 'dam': 1013}\n",
      "test_log {'not_a_dam': 258, 'dam': 1377}\n",
      "southaf_log {'not_a_dam': 28, 'dam': 96}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 8\n",
      "training_log {'not_a_dam': 903, 'dam': 4769}\n",
      "validation_log {'not_a_dam': 232, 'dam': 1129}\n",
      "test_log {'not_a_dam': 294, 'dam': 1529}\n",
      "southaf_log {'not_a_dam': 30, 'dam': 114}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 9\n",
      "training_log {'not_a_dam': 1011, 'dam': 5268}\n",
      "validation_log {'not_a_dam': 253, 'dam': 1252}\n",
      "test_log {'not_a_dam': 336, 'dam': 1720}\n",
      "southaf_log {'not_a_dam': 33, 'dam': 127}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 10\n",
      "training_log {'not_a_dam': 1110, 'dam': 5817}\n",
      "validation_log {'not_a_dam': 284, 'dam': 1359}\n",
      "test_log {'not_a_dam': 366, 'dam': 1889}\n",
      "southaf_log {'not_a_dam': 35, 'dam': 140}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 11\n",
      "training_log {'not_a_dam': 1216, 'dam': 6353}\n",
      "validation_log {'not_a_dam': 302, 'dam': 1482}\n",
      "test_log {'not_a_dam': 396, 'dam': 2052}\n",
      "southaf_log {'not_a_dam': 37, 'dam': 162}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 12\n",
      "training_log {'not_a_dam': 1310, 'dam': 6890}\n",
      "validation_log {'not_a_dam': 330, 'dam': 1608}\n",
      "test_log {'not_a_dam': 432, 'dam': 2211}\n",
      "southaf_log {'not_a_dam': 41, 'dam': 178}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 13\n",
      "training_log {'not_a_dam': 1414, 'dam': 7434}\n",
      "validation_log {'not_a_dam': 356, 'dam': 1724}\n",
      "test_log {'not_a_dam': 470, 'dam': 2371}\n",
      "southaf_log {'not_a_dam': 44, 'dam': 187}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 14\n",
      "training_log {'not_a_dam': 1527, 'dam': 7954}\n",
      "validation_log {'not_a_dam': 380, 'dam': 1852}\n",
      "test_log {'not_a_dam': 506, 'dam': 2533}\n",
      "southaf_log {'not_a_dam': 49, 'dam': 199}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 15\n",
      "training_log {'not_a_dam': 1629, 'dam': 8483}\n",
      "validation_log {'not_a_dam': 411, 'dam': 1991}\n",
      "test_log {'not_a_dam': 539, 'dam': 2686}\n",
      "southaf_log {'not_a_dam': 56, 'dam': 205}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 16\n",
      "training_log {'not_a_dam': 1738, 'dam': 9004}\n",
      "validation_log {'not_a_dam': 429, 'dam': 2120}\n",
      "test_log {'not_a_dam': 579, 'dam': 2855}\n",
      "southaf_log {'not_a_dam': 58, 'dam': 217}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 17\n",
      "training_log {'not_a_dam': 1841, 'dam': 9553}\n",
      "validation_log {'not_a_dam': 456, 'dam': 2245}\n",
      "test_log {'not_a_dam': 603, 'dam': 3014}\n",
      "southaf_log {'not_a_dam': 61, 'dam': 227}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 18\n",
      "training_log {'not_a_dam': 1951, 'dam': 10100}\n",
      "validation_log {'not_a_dam': 484, 'dam': 2359}\n",
      "test_log {'not_a_dam': 635, 'dam': 3166}\n",
      "southaf_log {'not_a_dam': 61, 'dam': 244}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 19\n",
      "training_log {'not_a_dam': 2046, 'dam': 10654}\n",
      "validation_log {'not_a_dam': 500, 'dam': 2475}\n",
      "test_log {'not_a_dam': 672, 'dam': 3324}\n",
      "southaf_log {'not_a_dam': 65, 'dam': 264}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 20\n",
      "training_log {'not_a_dam': 2147, 'dam': 11159}\n",
      "validation_log {'not_a_dam': 530, 'dam': 2598}\n",
      "test_log {'not_a_dam': 695, 'dam': 3525}\n",
      "southaf_log {'not_a_dam': 68, 'dam': 278}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 21\n",
      "training_log {'not_a_dam': 2265, 'dam': 11712}\n",
      "validation_log {'not_a_dam': 554, 'dam': 2707}\n",
      "test_log {'not_a_dam': 731, 'dam': 3667}\n",
      "southaf_log {'not_a_dam': 72, 'dam': 292}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 22\n",
      "training_log {'not_a_dam': 2366, 'dam': 12264}\n",
      "validation_log {'not_a_dam': 573, 'dam': 2826}\n",
      "test_log {'not_a_dam': 762, 'dam': 3835}\n",
      "southaf_log {'not_a_dam': 74, 'dam': 300}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 23\n",
      "training_log {'not_a_dam': 2474, 'dam': 12806}\n",
      "validation_log {'not_a_dam': 591, 'dam': 2946}\n",
      "test_log {'not_a_dam': 788, 'dam': 4001}\n",
      "southaf_log {'not_a_dam': 80, 'dam': 314}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 24\n",
      "training_log {'not_a_dam': 2589, 'dam': 13314}\n",
      "validation_log {'not_a_dam': 621, 'dam': 3080}\n",
      "test_log {'not_a_dam': 817, 'dam': 4169}\n",
      "southaf_log {'not_a_dam': 81, 'dam': 329}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 25\n",
      "training_log {'not_a_dam': 2706, 'dam': 13807}\n",
      "validation_log {'not_a_dam': 652, 'dam': 3210}\n",
      "test_log {'not_a_dam': 843, 'dam': 4355}\n",
      "southaf_log {'not_a_dam': 85, 'dam': 342}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 26\n",
      "training_log {'not_a_dam': 2806, 'dam': 14329}\n",
      "validation_log {'not_a_dam': 678, 'dam': 3334}\n",
      "test_log {'not_a_dam': 872, 'dam': 4534}\n",
      "southaf_log {'not_a_dam': 90, 'dam': 357}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 27\n",
      "training_log {'not_a_dam': 2921, 'dam': 14851}\n",
      "validation_log {'not_a_dam': 711, 'dam': 3462}\n",
      "test_log {'not_a_dam': 897, 'dam': 4693}\n",
      "southaf_log {'not_a_dam': 95, 'dam': 370}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 28\n",
      "training_log {'not_a_dam': 3013, 'dam': 15380}\n",
      "validation_log {'not_a_dam': 735, 'dam': 3600}\n",
      "test_log {'not_a_dam': 926, 'dam': 4863}\n",
      "southaf_log {'not_a_dam': 96, 'dam': 387}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 29\n",
      "training_log {'not_a_dam': 3110, 'dam': 15938}\n",
      "validation_log {'not_a_dam': 753, 'dam': 3718}\n",
      "test_log {'not_a_dam': 958, 'dam': 5028}\n",
      "southaf_log {'not_a_dam': 97, 'dam': 398}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 30\n",
      "training_log {'not_a_dam': 3198, 'dam': 16473}\n",
      "validation_log {'not_a_dam': 774, 'dam': 3834}\n",
      "test_log {'not_a_dam': 986, 'dam': 5223}\n",
      "southaf_log {'not_a_dam': 101, 'dam': 411}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 31\n",
      "training_log {'not_a_dam': 3287, 'dam': 17004}\n",
      "validation_log {'not_a_dam': 802, 'dam': 3961}\n",
      "test_log {'not_a_dam': 1020, 'dam': 5390}\n",
      "southaf_log {'not_a_dam': 104, 'dam': 432}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 32\n",
      "training_log {'not_a_dam': 3392, 'dam': 17533}\n",
      "validation_log {'not_a_dam': 827, 'dam': 4081}\n",
      "test_log {'not_a_dam': 1043, 'dam': 5571}\n",
      "southaf_log {'not_a_dam': 107, 'dam': 446}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 33\n",
      "training_log {'not_a_dam': 3500, 'dam': 18059}\n",
      "validation_log {'not_a_dam': 856, 'dam': 4211}\n",
      "test_log {'not_a_dam': 1079, 'dam': 5727}\n",
      "southaf_log {'not_a_dam': 108, 'dam': 460}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 34\n",
      "training_log {'not_a_dam': 3600, 'dam': 18624}\n",
      "validation_log {'not_a_dam': 882, 'dam': 4349}\n",
      "test_log {'not_a_dam': 1116, 'dam': 5853}\n",
      "southaf_log {'not_a_dam': 110, 'dam': 466}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 35\n",
      "training_log {'not_a_dam': 3697, 'dam': 19141}\n",
      "validation_log {'not_a_dam': 909, 'dam': 4463}\n",
      "test_log {'not_a_dam': 1140, 'dam': 6048}\n",
      "southaf_log {'not_a_dam': 113, 'dam': 489}\n",
      "\n",
      "\n",
      "Starting to make Darknet formated data, round 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_log {'not_a_dam': 3726, 'dam': 19264}\n",
      "validation_log {'not_a_dam': 914, 'dam': 4496}\n",
      "test_log {'not_a_dam': 1145, 'dam': 6087}\n",
      "southaf_log {'not_a_dam': 114, 'dam': 490}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make workspace outputs directories doesn't exist\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'southaf_set'),\n",
    "                      os.path.join(WORKSPACE_DIR,'southaf_set','labels'),\n",
    "                      os.path.join(WORKSPACE_DIR,'southaf_set','images')]\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    \n",
    "# Do the thing    \n",
    "training_log = {'dam': 0, 'not_a_dam': 0}\n",
    "validation_log = {'dam': 0, 'not_a_dam': 0}\n",
    "test_log = {'dam': 0, 'not_a_dam': 0}\n",
    "southaf_log = {'dam': 0, 'not_a_dam': 0}\n",
    "\n",
    "#last_time = time.time()\n",
    "\n",
    "iteration = 0\n",
    "max_iteration = int(len(all_images_file_list)/Dams_per_round)\n",
    "\n",
    "while iteration <= max_iteration:\n",
    "    \n",
    "    LOGGER.info('iteration %d' % iteration)\n",
    "\n",
    "    # Get a slice of the dataset\n",
    "    slice_dam_images_list = all_images_file_list[iteration*Dams_per_round:min((iteration+1)*Dams_per_round,len(all_images_file_list))]\n",
    "\n",
    "    #This is where I make Darknet_ready_inputs!\n",
    "    training_log, validation_log, test_log, southaf_log = Make_Darknet_inputs(slice_dam_images_list, iteration)\n",
    "\n",
    "    # Advance loop\n",
    "    iteration += 1\n",
    "\n",
    "    print('training_log',training_log)\n",
    "    print('validation_log',validation_log)\n",
    "    print('test_log',test_log)\n",
    "    print('southaf_log',southaf_log)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel listo\n"
     ]
    }
   ],
   "source": [
    "print('kernel listo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `dams.names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dams.names file\n"
     ]
    }
   ],
   "source": [
    "# create supporting files\n",
    "names = open(os.path.join(WORKSPACE_DIR,'dams.names'), 'w')\n",
    "names.write('dam')\n",
    "names.close()\n",
    "print('Wrote dams.names file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `dams.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 06-24.data file\n"
     ]
    }
   ],
   "source": [
    "## Writing .data file in inputs dir (WORKSPACE DIR)\n",
    "num_classes = 1\n",
    "filename = '06-24.data'\n",
    "\n",
    "data = open(os.path.join('cfg',filename), 'w')\n",
    "data.write('classes = '+ str(num_classes)\n",
    "           +' \\n train = ' + os.path.join(WORKSPACE_DIR,'training_set','training_image_filepaths.txt')\n",
    "           + '\\n valid = ' + os.path.join(WORKSPACE_DIR,'validation_set','validation_image_filepaths.txt')\n",
    "           +'\\n names =  ' + os.path.join(WORKSPACE_DIR,'dams.names')\n",
    "           +'\\n backup = ' + '../../../outputs/yolo_06-24/')\n",
    "data.close()\n",
    "\n",
    "print('Wrote '+filename+' file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dams.data file\n"
     ]
    }
   ],
   "source": [
    "## Writing .data file in cfg dir (WORKSPACE DIR)\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "data = open(os.path.join(WORKSPACE_DIR,'dams.data'), 'w')\n",
    "data.write('classes = '+ str(num_classes)\n",
    "           +' \\n train = ' + os.path.join(WORKSPACE_DIR,'training_set','training_image_filepaths.txt')\n",
    "           + '\\n valid = ' + os.path.join(WORKSPACE_DIR,'validation_set','validation_image_filepaths.txt')\n",
    "           +'\\n names =  ' + os.path.join(WORKSPACE_DIR,'dams.names')\n",
    "           +'\\n backup = backup/')\n",
    "data.close()\n",
    "\n",
    "print('Wrote dams.data file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create `[training, validation, test, southaf]_images_filepaths.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote validation_image_filepaths.txt\n",
      "Wrote test_image_filepaths.txt\n",
      "Wrote southaf_image_filepaths.txt\n",
      "Wrote training_image_filepaths.txt\n"
     ]
    }
   ],
   "source": [
    "for split in ['validation','test', 'southaf','training']:\n",
    "    split_set = split+'_set'\n",
    "\n",
    "    file_to_write = open(os.path.join(WORKSPACE_DIR,split_set,split+'_image_filepaths.txt'), 'w')\n",
    "\n",
    "    for image_name in os.listdir(os.path.join(WORKSPACE_DIR,split_set,'images')):\n",
    "        file_to_write.write(os.path.join(WORKSPACE_DIR,split_set,'images',image_name) + '\\n')\n",
    "    file_to_write.close()\n",
    "    \n",
    "    print('Wrote '+split+'_image_filepaths.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote validation_image_filepaths_rel.txt\n",
      "Wrote test_image_filepaths_rel.txt\n",
      "Wrote southaf_image_filepaths_rel.txt\n",
      "Wrote training_image_filepaths_rel.txt\n"
     ]
    }
   ],
   "source": [
    "# Relative paths don't work\n",
    "\n",
    "for split in ['validation','test', 'southaf','training']:\n",
    "    split_set = split+'_set'\n",
    "\n",
    "    file_to_write = open(os.path.join(WORKSPACE_DIR,split_set,split+'_image_filepaths_rel.txt'), 'w')\n",
    "\n",
    "    for image_name in os.listdir(os.path.join(WORKSPACE_DIR,split_set,'images')):\n",
    "        file_to_write.write(os.path.join('images',image_name) + '\\n')\n",
    "    file_to_write.close()\n",
    "    \n",
    "    print('Wrote '+split+'_image_filepaths_rel.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote validation_image_filepaths_abs.txt\n",
      "Wrote test_image_filepaths_abs.txt\n",
      "Wrote southaf_image_filepaths_abs.txt\n",
      "Wrote training_image_filepaths_abs.txt\n"
     ]
    }
   ],
   "source": [
    "# Absolute paths\n",
    "absolute_path_to_data = '/home/adminatcap/charlie/data/Darknet-inputs_imagery_6-7_made_6-21/' \n",
    "\n",
    "for split in ['validation','test', 'southaf','training']:\n",
    "    split_set = split+'_set'\n",
    "\n",
    "    file_to_write = open(os.path.join(absolute_path_to_data,split_set,split+'_image_filepaths_abs.txt'), 'w')\n",
    "\n",
    "    for image_name in os.listdir(os.path.join(absolute_path_to_data,split_set,'images')):\n",
    "        file_to_write.write(os.path.join(absolute_path_to_data,split_set,'images',image_name) + '\\n')\n",
    "    file_to_write.close()\n",
    "    \n",
    "    print('Wrote '+split+'_image_filepaths_abs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
