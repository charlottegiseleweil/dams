{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script transform raw imagery (in not_a_dam_images and dam_images folders) to Darknet-formatted inputs for YOLOv3 (link to implementation Maanas?)\n",
    "\n",
    "Outputs in 2 directories:\n",
    "* images: has filename.png\n",
    "* labels: filename.txt : class, center x, center y, width, height (x y w h are all normalized (between 0 and 1) relative to image dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "import numpy as np\n",
    "import shapely\n",
    "import random\n",
    "from matplotlib import image as mpimg\n",
    "\n",
    "import gdal\n",
    "\n",
    "import requests\n",
    "## Useless?\n",
    "from osgeo import gdal\n",
    "import ogr\n",
    "import shapely.wkb\n",
    "import shapely.prepared\n",
    "\n",
    "import logging\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "LOGGER.info(\"Logger in INFO mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "\n",
    "REQUEST_TIMEOUT = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Paths\n",
    "\n",
    "path_to_charlie_root = \"../../..\"\n",
    "NOT_A_DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/not_a_dam_images\")\n",
    "DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/dam_images\")\n",
    "\n",
    "TM_WORLD_BORDERS_URL = 'https://storage.googleapis.com/ecoshard-root/ipbes/TM_WORLD_BORDERS_SIMPL-0.3_md5_15057f7b17752048f9bd2e2e607fe99c.zip'\n",
    "\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "if not os.path.exists(DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % DAM_IMAGE_DIR)\n",
    "    \n",
    "#OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/YOLOready_imagery_6-7_made_6-21\")\n",
    "OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/YOLOready_imagery_6-7_test\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just one of the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsets of data inputs - for faster development purposes\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('5140_clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('362_not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full dataset \n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do run this one to merge dam_list and not_a_dam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_file_list = dam_file_list+not_a_dam_file_list\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(all_images_file_list)\n",
    "\n",
    "len(all_images_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set_portion = .2\n",
    "validation_set_portion = .15\n",
    "Dams_per_round = 5#1000 # = max_dams_per_record "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get South Africa geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url_to_file(url, target_file_path):\n",
    "    \"\"\"Use requests to download a file.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): url to file.\n",
    "        target_file_path (string): local path to download the file.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(target_file_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "        with open(target_file_path, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response.raw, target_file)\n",
    "        del response\n",
    "    except:\n",
    "        LOGGER.exception('download of {url} to {target_file_path} failed')\n",
    "        # mods from LOGGER.exception(f'download of {url} to {target_file_path} failed')\n",
    "        raise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "tm_world_borders_zip_path = os.path.join(\n",
    "        WORKSPACE_DIR, 'world_borders', os.path.basename(TM_WORLD_BORDERS_URL))\n",
    "if not os.path.exists(tm_world_borders_zip_path):\n",
    "    download_url_to_file(TM_WORLD_BORDERS_URL, tm_world_borders_zip_path)\n",
    "    with zipfile.ZipFile(tm_world_borders_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(os.path.join(WORKSPACE_DIR,'world_borders'))\n",
    "\n",
    "tm_world_borders_vector_path = os.path.join(\n",
    "    WORKSPACE_DIR,'world_borders', 'TM_WORLD_BORDERS-0.3.shp')\n",
    "\n",
    "tm_world_borders_vector = ogr.Open(tm_world_borders_vector_path)\n",
    "tm_world_borders_layer = tm_world_borders_vector.GetLayer()\n",
    "for border_feature in tm_world_borders_layer:\n",
    "    if border_feature.GetField('NAME') == 'South Africa':\n",
    "        sa_geom = border_feature.GetGeometryRef()\n",
    "        sa_geom_prep = shapely.prepared.prep(\n",
    "            shapely.wkb.loads(sa_geom.ExportToWkb()))\n",
    "        break\n",
    "LOGGER.debug(sa_geom_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Make YOLO-ready data !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: Function to make YOLO_ready data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Make_Darknet_inputs(images_file_list, iteration): \n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    Takes in folder of dam pngs, folder of bounding box json files\n",
    "    Normalizes json data to darknet format (center x, center y, bbox width, bbox height)\n",
    "    Creates new directories in darknet format\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Starting to make YOLO-ready data, round %d' % iteration)\n",
    "    \n",
    "    random.seed(iteration)\n",
    "    random_numbers_list = [random.random() for x in range(0, len(images_file_list))]\n",
    "    random_number_iterator = 0\n",
    "    \n",
    "    for image_path in images_file_list:\n",
    "        \n",
    "        # read in image\n",
    "        img = mpimg.imread(image_path)\n",
    "\n",
    "        # get width and height\n",
    "        img_w = img.shape[0]\n",
    "        img_h = img.shape[1]\n",
    "\n",
    "        # get matching bounding box json file\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if not os.path.exists(json_path):\n",
    "            raise NameError(\"can't find bbox for %s\" % json_path)\n",
    "            \n",
    "        # read json bounding box coordinates\n",
    "        with open(json_path, 'r') as json_file: \n",
    "            image_metadata = json.load(json_file)\n",
    "        \n",
    "        # normalize to x-center, y-center, width, and height of bbox\n",
    "        coords = image_metadata['pixel_bounding_box']\n",
    "        avg_x = (coords[2] + coords[0]) / (2 * img_w)\n",
    "        avg_y = (coords[1] + coords[3]) / (2 * img_h)\n",
    "        nrm_w = (coords[2] - coords[0]) / img_w\n",
    "        nrm_h = (coords[1] - coords[3]) / img_h\n",
    "        nrm_xywh = np.array([avg_x, avg_y, nrm_w, nrm_h])\n",
    "\n",
    "        # Define new label in YOLO format\n",
    "        if 'not_a_dam' in image_path:\n",
    "            dam_type = 'not_a_dam'\n",
    "            label_str = ''\n",
    "        else:\n",
    "            dam_type = 'dam'\n",
    "            label_str = '0 ' + str('%.6f'%nrm_xywh[0]) + ' ' + str('%.6f'%nrm_xywh[1]) + ' ' + str('%.6f'%nrm_xywh[2]) + ' ' + str('%.6f'%nrm_xywh[3])\n",
    "\n",
    "            \n",
    "            \n",
    "        # - - -   - - -   - - -   \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "            \n",
    "            \n",
    "        random_number = random_numbers_list[random_number_iterator]\n",
    "        random_number_iterator+=1\n",
    "        \n",
    "        if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # both for dams & not_a_dams\n",
    "            writer = 'southaf_set'\n",
    "            log = southaf_log\n",
    "        elif random_number < holdout_set_portion:\n",
    "            writer = 'test_set'\n",
    "            log = test_log\n",
    "        elif random_number > (1-validation_set_portion):\n",
    "            writer = 'validation_set'\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = 'training_set'\n",
    "            log = training_log\n",
    "            \n",
    "        # Write the file in the corresponding set\n",
    "        \n",
    "        ## Write image here:\n",
    "        filename = image_path.split(\"images/\")[1].replace('.png','')\n",
    "            \n",
    "        newimg_filepath = os.path.join(OUTPUTS_DIR,writer,'images',filename+'.png')\n",
    "        shutil.copyfile(image_path, newimg_filepath)\n",
    "        \n",
    "        \n",
    "        newtext_filepath = os.path.join(OUTPUTS_DIR,writer,'labels',filename+'.txt')\n",
    "        file = open(newtext_filepath, 'w')\n",
    "        file.write(label_str)\n",
    "        file.close()\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "        \n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make workspace outputs directories doesn't exist\n",
    "\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set','labels'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set','images'),\n",
    "                       os.path.join(WORKSPACE_DIR,'southaf_set'),\n",
    "                      os.path.join(WORKSPACE_DIR,'southaf_set','labels'),\n",
    "                      os.path.join(WORKSPACE_DIR,'southaf_set','images')]\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        \n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make YOLO-ready data, round 0\n",
      "training_log {'dam': 1, 'not_a_dam': 4}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 0, 'not_a_dam': 0}\n",
      "\n",
      "\n",
      "Starting to make YOLO-ready data, round 1\n",
      "training_log {'dam': 2, 'not_a_dam': 6}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 1}\n",
      "southaf_log {'dam': 0, 'not_a_dam': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do the thing    \n",
    "training_log = {'dam': 0, 'not_a_dam': 0}\n",
    "validation_log = {'dam': 0, 'not_a_dam': 0}\n",
    "test_log = {'dam': 0, 'not_a_dam': 0}\n",
    "southaf_log = {'dam': 0, 'not_a_dam': 0}\n",
    "\n",
    "#last_time = time.time()\n",
    "\n",
    "iteration = 0\n",
    "max_iteration = int(len(all_images_file_list)/Dams_per_round)\n",
    "\n",
    "while iteration <= max_iteration:\n",
    "    \n",
    "    LOGGER.info('iteration %d' % iteration)\n",
    "\n",
    "    # Get a slice of the dataset\n",
    "    slice_dam_images_list = all_images_file_list[iteration*Dams_per_round:min((iteration+1)*Dams_per_round,len(all_images_file_list))]\n",
    "\n",
    "    #This is where I make Darknet_ready_inputs!\n",
    "    training_log, validation_log, test_log, southaf_log = Make_Darknet_inputs(slice_dam_images_list, iteration)\n",
    "\n",
    "    # Advance loop\n",
    "    iteration += 1\n",
    "\n",
    "    print('training_log',training_log)\n",
    "    print('validation_log',validation_log)\n",
    "    print('test_log',test_log)\n",
    "    print('southaf_log',southaf_log)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
