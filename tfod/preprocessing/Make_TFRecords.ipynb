{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning sunshine\n"
     ]
    }
   ],
   "source": [
    "print('Good morning sunshine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requires TF + gdal + pip install request, shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# Installed in addition\n",
    "#import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gdal\n",
    "import ogr\n",
    "import shapely.wkb\n",
    "import shapely.prepared\n",
    "#from retrying import retry\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.3\n"
     ]
    }
   ],
   "source": [
    "import osgeo\n",
    "print(osgeo.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "\n",
    "import requests\n",
    "\n",
    "import logging\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "LOGGER.info(\"Logger in INFO mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "\n",
    "REQUEST_TIMEOUT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=('%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n",
    "                            ' [%(funcName)s:%(lineno)d] %(message)s'),\n",
    "                    stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_charlie_root = \"../../../..\"\n",
    "NOT_A_DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-7-25_cropped_419/not_a_dam_images\")\n",
    "DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-7-25_cropped_419/dam_images\")\n",
    "\n",
    "TM_WORLD_BORDERS_URL = 'https://storage.googleapis.com/ecoshard-root/ipbes/TM_WORLD_BORDERS_SIMPL-0.3_md5_15057f7b17752048f9bd2e2e607fe99c.zip'\n",
    "\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "if not os.path.exists(DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % DAM_IMAGE_DIR)\n",
    "    \n",
    "OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/TFRecords_imagery_6-7_made_6-24\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just one of the 2 cells below! (full dataset or subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsets of data inputs - for faster development purposes\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('5140_clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('362_not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full dataset \n",
    "\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('not_a_dam.png')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42656"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do run this one to merge dam_list and not_a_dam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48555"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_file_list = dam_file_list+not_a_dam_file_list\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(all_images_file_list)\n",
    "len(all_images_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the computation of the full dataset crashes, let's do it bits by bits, pickling the list of paths so they stay in order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work in progressss\n",
    "\n",
    "with open(\"list_of_paths.txt\", \"wb\") as file:\n",
    "    pickle.dump(all_images_file_list, file)\n",
    "... \n",
    ">>> with open(\"list_of_paths.txt\", \"rb\") as file:   # Unpickling\n",
    "...   all_images_file_list = pickle.load(file)\n",
    "... \n",
    ">>> b\n",
    "[1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set_portion = .2\n",
    "validation_set_portion = .15\n",
    "Dams_per_round = 1000 # = max_dams_per_record \n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get South Africa geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url_to_file(url, target_file_path):\n",
    "    \"\"\"Use requests to download a file.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): url to file.\n",
    "        target_file_path (string): local path to download the file.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(target_file_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "        with open(target_file_path, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response.raw, target_file)\n",
    "        del response\n",
    "    except:\n",
    "        LOGGER.exception('download of {url} to {target_file_path} failed')\n",
    "        # mods from LOGGER.exception(f'download of {url} to {target_file_path} failed')\n",
    "        raise\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "tm_world_borders_zip_path = os.path.join(\n",
    "        WORKSPACE_DIR, 'world_borders', os.path.basename(TM_WORLD_BORDERS_URL))\n",
    "if not os.path.exists(tm_world_borders_zip_path):\n",
    "    download_url_to_file(TM_WORLD_BORDERS_URL, tm_world_borders_zip_path)\n",
    "    with zipfile.ZipFile(tm_world_borders_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(WORKSPACE_DIR)\n",
    "\n",
    "tm_world_borders_vector_path = os.path.join(\n",
    "    WORKSPACE_DIR, 'TM_WORLD_BORDERS-0.3.shp')\n",
    "#tm_world_borders_vector = gdal.Open(ogr.Open(tm_world_borders_vector_path)) # Changed OpenEx to Open.\n",
    "    #tm_world_borders_vector_path,ogr.Open(path))#, gdal.OF_VECTOR)\n",
    "tm_world_borders_vector = ogr.Open(tm_world_borders_vector_path)\n",
    "tm_world_borders_layer = tm_world_borders_vector.GetLayer()\n",
    "for border_feature in tm_world_borders_layer:\n",
    "    if border_feature.GetField('NAME') == 'South Africa':\n",
    "        sa_geom = border_feature.GetGeometryRef()\n",
    "        sa_geom_prep = shapely.prepared.prep(\n",
    "            shapely.wkb.loads(sa_geom.ExportToWkb()))\n",
    "        break\n",
    "LOGGER.debug(sa_geom_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Make TFRecords !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: Function to make_TFRecords from a list of images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords(dam_file_list,tf_record_iteration):\n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "    random.seed(tf_record_iteration)\n",
    "    random_numbers_list = [random.random() for x in range(0, len(dam_file_list))]\n",
    "    random_number_iterator = 0\n",
    "    \n",
    "    for image_path in dam_file_list:\n",
    "#             current_time = time.time()\n",
    "#             if current_time - last_time > 5.0:\n",
    "#                 LOGGER.info('training_log: %s', training_log)\n",
    "#                 LOGGER.info('validation_log: %s', validation_log)\n",
    "#                 LOGGER.info('southaf_log: %s', southaf_log)\n",
    "#                 LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "#                 LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "#                 last_time = current_time\n",
    "\n",
    "        #  Note from Rich:\n",
    "        # looks like anything can be used here, including serializing\n",
    "        # a tensor tf.serialize_tensor\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "\n",
    "        # if this image is a dam:\n",
    "\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if not 'not_a_dam' in image_path:\n",
    "            dam_type = 'dam'\n",
    "\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax > 1 or ymax > 1):\n",
    "                #LOGGER.warning('bounding box out of bounds %s %s %s %s',\n",
    "                #               xmin, xmax, ymin, ymax)\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = 'not_a_dam'\n",
    "            \n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            \n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "\n",
    "        \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "            \n",
    "            \n",
    "        random_number = random_numbers_list[random_number_iterator]\n",
    "        random_number_iterator+=1\n",
    "        \n",
    "        if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # both for dams & not_a_dams\n",
    "            writer = southaf_writer\n",
    "            log = southaf_log\n",
    "        elif random_number < holdout_set_portion:\n",
    "            writer = test_writer\n",
    "            log = test_log\n",
    "        elif random_number > (1-validation_set_portion):\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-29 04:40:32,637 (178347) INFO root [<module>:33] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {'dam': 500, 'not_a_dam': 84}\n",
      "validation_log {'dam': 109, 'not_a_dam': 21}\n",
      "test_log {'dam': 163, 'not_a_dam': 20}\n",
      "southaf_log {'dam': 102, 'not_a_dam': 1}\n",
      "\n",
      "\n",
      "2019-07-29 04:41:16,198 (221909) INFO root [<module>:33] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {'dam': 986, 'not_a_dam': 171}\n",
      "validation_log {'dam': 236, 'not_a_dam': 40}\n",
      "test_log {'dam': 293, 'not_a_dam': 51}\n",
      "southaf_log {'dam': 217, 'not_a_dam': 6}\n",
      "\n",
      "\n",
      "2019-07-29 04:42:01,135 (266846) INFO root [<module>:33] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {'dam': 1489, 'not_a_dam': 244}\n",
      "validation_log {'dam': 352, 'not_a_dam': 58}\n",
      "test_log {'dam': 443, 'not_a_dam': 80}\n",
      "southaf_log {'dam': 326, 'not_a_dam': 8}\n",
      "\n",
      "\n",
      "2019-07-29 04:42:49,633 (315344) INFO root [<module>:33] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {'dam': 1976, 'not_a_dam': 324}\n",
      "validation_log {'dam': 466, 'not_a_dam': 78}\n",
      "test_log {'dam': 601, 'not_a_dam': 113}\n",
      "southaf_log {'dam': 431, 'not_a_dam': 11}\n",
      "\n",
      "\n",
      "2019-07-29 04:43:39,807 (365518) INFO root [<module>:33] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {'dam': 2453, 'not_a_dam': 384}\n",
      "validation_log {'dam': 600, 'not_a_dam': 98}\n",
      "test_log {'dam': 770, 'not_a_dam': 135}\n",
      "southaf_log {'dam': 545, 'not_a_dam': 15}\n",
      "\n",
      "\n",
      "2019-07-29 04:44:28,731 (414441) INFO root [<module>:33] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {'dam': 2947, 'not_a_dam': 467}\n",
      "validation_log {'dam': 708, 'not_a_dam': 119}\n",
      "test_log {'dam': 940, 'not_a_dam': 155}\n",
      "southaf_log {'dam': 648, 'not_a_dam': 16}\n",
      "\n",
      "\n",
      "2019-07-29 04:45:19,754 (465464) INFO root [<module>:33] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {'dam': 3414, 'not_a_dam': 536}\n",
      "validation_log {'dam': 831, 'not_a_dam': 136}\n",
      "test_log {'dam': 1102, 'not_a_dam': 182}\n",
      "southaf_log {'dam': 780, 'not_a_dam': 19}\n",
      "\n",
      "\n",
      "2019-07-29 04:46:08,281 (513992) INFO root [<module>:33] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {'dam': 3906, 'not_a_dam': 620}\n",
      "validation_log {'dam': 943, 'not_a_dam': 150}\n",
      "test_log {'dam': 1263, 'not_a_dam': 206}\n",
      "southaf_log {'dam': 891, 'not_a_dam': 21}\n",
      "\n",
      "\n",
      "2019-07-29 04:46:57,471 (563181) INFO root [<module>:33] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {'dam': 4417, 'not_a_dam': 690}\n",
      "validation_log {'dam': 1051, 'not_a_dam': 175}\n",
      "test_log {'dam': 1414, 'not_a_dam': 226}\n",
      "southaf_log {'dam': 1006, 'not_a_dam': 21}\n",
      "\n",
      "\n",
      "2019-07-29 04:47:46,246 (611956) INFO root [<module>:33] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {'dam': 4882, 'not_a_dam': 765}\n",
      "validation_log {'dam': 1163, 'not_a_dam': 196}\n",
      "test_log {'dam': 1592, 'not_a_dam': 261}\n",
      "southaf_log {'dam': 1116, 'not_a_dam': 25}\n",
      "\n",
      "\n",
      "2019-07-29 04:48:36,512 (662222) INFO root [<module>:33] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {'dam': 5387, 'not_a_dam': 846}\n",
      "validation_log {'dam': 1273, 'not_a_dam': 213}\n",
      "test_log {'dam': 1754, 'not_a_dam': 282}\n",
      "southaf_log {'dam': 1219, 'not_a_dam': 26}\n",
      "\n",
      "\n",
      "2019-07-29 04:49:22,741 (708451) INFO root [<module>:33] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n",
      "training_log {'dam': 5911, 'not_a_dam': 921}\n",
      "validation_log {'dam': 1389, 'not_a_dam': 229}\n",
      "test_log {'dam': 1904, 'not_a_dam': 304}\n",
      "southaf_log {'dam': 1313, 'not_a_dam': 29}\n",
      "\n",
      "\n",
      "2019-07-29 04:50:11,537 (757248) INFO root [<module>:33] tf_record_iteration 12\n",
      "Starting to make TFRecords 12\n",
      "training_log {'dam': 6420, 'not_a_dam': 991}\n",
      "validation_log {'dam': 1507, 'not_a_dam': 249}\n",
      "test_log {'dam': 2060, 'not_a_dam': 324}\n",
      "southaf_log {'dam': 1416, 'not_a_dam': 33}\n",
      "\n",
      "\n",
      "2019-07-29 04:50:58,780 (804490) INFO root [<module>:33] tf_record_iteration 13\n",
      "Starting to make TFRecords 13\n",
      "training_log {'dam': 6923, 'not_a_dam': 1071}\n",
      "validation_log {'dam': 1607, 'not_a_dam': 268}\n",
      "test_log {'dam': 2224, 'not_a_dam': 344}\n",
      "southaf_log {'dam': 1526, 'not_a_dam': 37}\n",
      "\n",
      "\n",
      "2019-07-29 04:51:48,048 (853759) INFO root [<module>:33] tf_record_iteration 14\n",
      "Starting to make TFRecords 14\n",
      "training_log {'dam': 7419, 'not_a_dam': 1150}\n",
      "validation_log {'dam': 1734, 'not_a_dam': 289}\n",
      "test_log {'dam': 2379, 'not_a_dam': 365}\n",
      "southaf_log {'dam': 1626, 'not_a_dam': 38}\n",
      "\n",
      "\n",
      "2019-07-29 04:52:34,183 (899894) INFO root [<module>:33] tf_record_iteration 15\n",
      "Starting to make TFRecords 15\n",
      "training_log {'dam': 7924, 'not_a_dam': 1231}\n",
      "validation_log {'dam': 1865, 'not_a_dam': 313}\n",
      "test_log {'dam': 2516, 'not_a_dam': 398}\n",
      "southaf_log {'dam': 1712, 'not_a_dam': 41}\n",
      "\n",
      "\n",
      "2019-07-29 04:53:23,376 (949087) INFO root [<module>:33] tf_record_iteration 16\n",
      "Starting to make TFRecords 16\n",
      "training_log {'dam': 8426, 'not_a_dam': 1301}\n",
      "validation_log {'dam': 1983, 'not_a_dam': 324}\n",
      "test_log {'dam': 2679, 'not_a_dam': 417}\n",
      "southaf_log {'dam': 1825, 'not_a_dam': 45}\n",
      "\n",
      "\n",
      "2019-07-29 04:54:09,877 (995588) INFO root [<module>:33] tf_record_iteration 17\n",
      "Starting to make TFRecords 17\n",
      "training_log {'dam': 8945, 'not_a_dam': 1385}\n",
      "validation_log {'dam': 2098, 'not_a_dam': 344}\n",
      "test_log {'dam': 2821, 'not_a_dam': 436}\n",
      "southaf_log {'dam': 1924, 'not_a_dam': 47}\n",
      "\n",
      "\n",
      "2019-07-29 04:54:58,361 (1044071) INFO root [<module>:33] tf_record_iteration 18\n",
      "Starting to make TFRecords 18\n",
      "training_log {'dam': 9461, 'not_a_dam': 1473}\n",
      "validation_log {'dam': 2215, 'not_a_dam': 364}\n",
      "test_log {'dam': 2973, 'not_a_dam': 456}\n",
      "southaf_log {'dam': 2010, 'not_a_dam': 48}\n",
      "\n",
      "\n",
      "2019-07-29 04:55:44,977 (1090687) INFO root [<module>:33] tf_record_iteration 19\n",
      "Starting to make TFRecords 19\n",
      "training_log {'dam': 9977, 'not_a_dam': 1544}\n",
      "validation_log {'dam': 2323, 'not_a_dam': 377}\n",
      "test_log {'dam': 3133, 'not_a_dam': 474}\n",
      "southaf_log {'dam': 2121, 'not_a_dam': 51}\n",
      "\n",
      "\n",
      "2019-07-29 04:56:30,979 (1136690) INFO root [<module>:33] tf_record_iteration 20\n",
      "Starting to make TFRecords 20\n",
      "training_log {'dam': 10448, 'not_a_dam': 1615}\n",
      "validation_log {'dam': 2442, 'not_a_dam': 393}\n",
      "test_log {'dam': 3316, 'not_a_dam': 495}\n",
      "southaf_log {'dam': 2238, 'not_a_dam': 53}\n",
      "\n",
      "\n",
      "2019-07-29 04:57:17,234 (1182945) INFO root [<module>:33] tf_record_iteration 21\n",
      "Starting to make TFRecords 21\n",
      "training_log {'dam': 10983, 'not_a_dam': 1683}\n",
      "validation_log {'dam': 2539, 'not_a_dam': 408}\n",
      "test_log {'dam': 3456, 'not_a_dam': 519}\n",
      "southaf_log {'dam': 2356, 'not_a_dam': 56}\n",
      "\n",
      "\n",
      "2019-07-29 04:58:04,896 (1230606) INFO root [<module>:33] tf_record_iteration 22\n",
      "Starting to make TFRecords 22\n",
      "training_log {'dam': 11494, 'not_a_dam': 1771}\n",
      "validation_log {'dam': 2633, 'not_a_dam': 435}\n",
      "test_log {'dam': 3610, 'not_a_dam': 543}\n",
      "southaf_log {'dam': 2458, 'not_a_dam': 56}\n",
      "\n",
      "\n",
      "2019-07-29 04:58:50,764 (1276475) INFO root [<module>:33] tf_record_iteration 23\n",
      "Starting to make TFRecords 23\n",
      "training_log {'dam': 12012, 'not_a_dam': 1826}\n",
      "validation_log {'dam': 2745, 'not_a_dam': 450}\n",
      "test_log {'dam': 3767, 'not_a_dam': 571}\n",
      "southaf_log {'dam': 2571, 'not_a_dam': 58}\n",
      "\n",
      "\n",
      "2019-07-29 04:59:36,451 (1322162) INFO root [<module>:33] tf_record_iteration 24\n",
      "Starting to make TFRecords 24\n",
      "training_log {'dam': 12501, 'not_a_dam': 1898}\n",
      "validation_log {'dam': 2875, 'not_a_dam': 471}\n",
      "test_log {'dam': 3926, 'not_a_dam': 591}\n",
      "southaf_log {'dam': 2678, 'not_a_dam': 60}\n",
      "\n",
      "\n",
      "2019-07-29 05:00:22,737 (1368447) INFO root [<module>:33] tf_record_iteration 25\n",
      "Starting to make TFRecords 25\n",
      "training_log {'dam': 12988, 'not_a_dam': 1969}\n",
      "validation_log {'dam': 3006, 'not_a_dam': 487}\n",
      "test_log {'dam': 4097, 'not_a_dam': 616}\n",
      "southaf_log {'dam': 2774, 'not_a_dam': 63}\n",
      "\n",
      "\n",
      "2019-07-29 05:01:09,573 (1415284) INFO root [<module>:33] tf_record_iteration 26\n",
      "Starting to make TFRecords 26\n",
      "training_log {'dam': 13475, 'not_a_dam': 2053}\n",
      "validation_log {'dam': 3109, 'not_a_dam': 519}\n",
      "test_log {'dam': 4262, 'not_a_dam': 641}\n",
      "southaf_log {'dam': 2877, 'not_a_dam': 64}\n",
      "\n",
      "\n",
      "2019-07-29 05:01:55,266 (1460976) INFO root [<module>:33] tf_record_iteration 27\n",
      "Starting to make TFRecords 27\n",
      "training_log {'dam': 13965, 'not_a_dam': 2142}\n",
      "validation_log {'dam': 3241, 'not_a_dam': 537}\n",
      "test_log {'dam': 4410, 'not_a_dam': 668}\n",
      "southaf_log {'dam': 2973, 'not_a_dam': 64}\n",
      "\n",
      "\n",
      "2019-07-29 05:02:43,145 (1508856) INFO root [<module>:33] tf_record_iteration 28\n",
      "Starting to make TFRecords 28\n",
      "training_log {'dam': 14464, 'not_a_dam': 2215}\n",
      "validation_log {'dam': 3360, 'not_a_dam': 554}\n",
      "test_log {'dam': 4571, 'not_a_dam': 695}\n",
      "southaf_log {'dam': 3074, 'not_a_dam': 67}\n",
      "\n",
      "\n",
      "2019-07-29 05:03:27,634 (1553344) INFO root [<module>:33] tf_record_iteration 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make TFRecords 29\n",
      "training_log {'dam': 14960, 'not_a_dam': 2297}\n",
      "validation_log {'dam': 3469, 'not_a_dam': 569}\n",
      "test_log {'dam': 4724, 'not_a_dam': 719}\n",
      "southaf_log {'dam': 3191, 'not_a_dam': 71}\n",
      "\n",
      "\n",
      "2019-07-29 05:04:15,442 (1601152) INFO root [<module>:33] tf_record_iteration 30\n",
      "Starting to make TFRecords 30\n",
      "training_log {'dam': 15455, 'not_a_dam': 2373}\n",
      "validation_log {'dam': 3573, 'not_a_dam': 581}\n",
      "test_log {'dam': 4899, 'not_a_dam': 745}\n",
      "southaf_log {'dam': 3301, 'not_a_dam': 73}\n",
      "\n",
      "\n",
      "2019-07-29 05:05:01,367 (1647078) INFO root [<module>:33] tf_record_iteration 31\n",
      "Starting to make TFRecords 31\n",
      "training_log {'dam': 15941, 'not_a_dam': 2451}\n",
      "validation_log {'dam': 3699, 'not_a_dam': 603}\n",
      "test_log {'dam': 5062, 'not_a_dam': 769}\n",
      "southaf_log {'dam': 3398, 'not_a_dam': 77}\n",
      "\n",
      "\n",
      "2019-07-29 05:05:50,126 (1695836) INFO root [<module>:33] tf_record_iteration 32\n",
      "Starting to make TFRecords 32\n",
      "training_log {'dam': 16416, 'not_a_dam': 2543}\n",
      "validation_log {'dam': 3813, 'not_a_dam': 617}\n",
      "test_log {'dam': 5231, 'not_a_dam': 793}\n",
      "southaf_log {'dam': 3508, 'not_a_dam': 79}\n",
      "\n",
      "\n",
      "2019-07-29 05:06:33,888 (1739598) INFO root [<module>:33] tf_record_iteration 33\n",
      "Starting to make TFRecords 33\n",
      "training_log {'dam': 16919, 'not_a_dam': 2602}\n",
      "validation_log {'dam': 3930, 'not_a_dam': 644}\n",
      "test_log {'dam': 5388, 'not_a_dam': 814}\n",
      "southaf_log {'dam': 3622, 'not_a_dam': 81}\n",
      "\n",
      "\n",
      "2019-07-29 05:07:21,782 (1787493) INFO root [<module>:33] tf_record_iteration 34\n",
      "Starting to make TFRecords 34\n",
      "training_log {'dam': 17440, 'not_a_dam': 2695}\n",
      "validation_log {'dam': 4063, 'not_a_dam': 662}\n",
      "test_log {'dam': 5512, 'not_a_dam': 836}\n",
      "southaf_log {'dam': 3711, 'not_a_dam': 81}\n",
      "\n",
      "\n",
      "2019-07-29 05:08:07,182 (1832893) INFO root [<module>:33] tf_record_iteration 35\n",
      "Starting to make TFRecords 35\n",
      "training_log {'dam': 17917, 'not_a_dam': 2772}\n",
      "validation_log {'dam': 4172, 'not_a_dam': 678}\n",
      "test_log {'dam': 5684, 'not_a_dam': 861}\n",
      "southaf_log {'dam': 3833, 'not_a_dam': 83}\n",
      "\n",
      "\n",
      "2019-07-29 05:08:53,839 (1879550) INFO root [<module>:33] tf_record_iteration 36\n",
      "Starting to make TFRecords 36\n",
      "training_log {'dam': 18397, 'not_a_dam': 2853}\n",
      "validation_log {'dam': 4309, 'not_a_dam': 691}\n",
      "test_log {'dam': 5843, 'not_a_dam': 880}\n",
      "southaf_log {'dam': 3942, 'not_a_dam': 85}\n",
      "\n",
      "\n",
      "2019-07-29 05:09:40,838 (1926549) INFO root [<module>:33] tf_record_iteration 37\n",
      "Starting to make TFRecords 37\n",
      "training_log {'dam': 18910, 'not_a_dam': 2931}\n",
      "validation_log {'dam': 4430, 'not_a_dam': 707}\n",
      "test_log {'dam': 5988, 'not_a_dam': 909}\n",
      "southaf_log {'dam': 4039, 'not_a_dam': 86}\n",
      "\n",
      "\n",
      "2019-07-29 05:10:25,906 (1971617) INFO root [<module>:33] tf_record_iteration 38\n",
      "Starting to make TFRecords 38\n",
      "training_log {'dam': 19397, 'not_a_dam': 3009}\n",
      "validation_log {'dam': 4553, 'not_a_dam': 726}\n",
      "test_log {'dam': 6147, 'not_a_dam': 929}\n",
      "southaf_log {'dam': 4151, 'not_a_dam': 88}\n",
      "\n",
      "\n",
      "2019-07-29 05:11:11,612 (2017322) INFO root [<module>:33] tf_record_iteration 39\n",
      "Starting to make TFRecords 39\n",
      "training_log {'dam': 19885, 'not_a_dam': 3096}\n",
      "validation_log {'dam': 4665, 'not_a_dam': 741}\n",
      "test_log {'dam': 6308, 'not_a_dam': 965}\n",
      "southaf_log {'dam': 4249, 'not_a_dam': 91}\n",
      "\n",
      "\n",
      "2019-07-29 05:11:58,986 (2064696) INFO root [<module>:33] tf_record_iteration 40\n",
      "Starting to make TFRecords 40\n",
      "training_log {'dam': 20402, 'not_a_dam': 3159}\n",
      "validation_log {'dam': 4757, 'not_a_dam': 759}\n",
      "test_log {'dam': 6484, 'not_a_dam': 988}\n",
      "southaf_log {'dam': 4356, 'not_a_dam': 95}\n",
      "\n",
      "\n",
      "2019-07-29 05:12:45,818 (2111528) INFO root [<module>:33] tf_record_iteration 41\n",
      "Starting to make TFRecords 41\n",
      "training_log {'dam': 20919, 'not_a_dam': 3227}\n",
      "validation_log {'dam': 4884, 'not_a_dam': 773}\n",
      "test_log {'dam': 6615, 'not_a_dam': 1017}\n",
      "southaf_log {'dam': 4466, 'not_a_dam': 99}\n",
      "\n",
      "\n",
      "2019-07-29 05:13:30,569 (2156279) INFO root [<module>:33] tf_record_iteration 42\n",
      "Starting to make TFRecords 42\n",
      "training_log {'dam': 21424, 'not_a_dam': 3314}\n",
      "validation_log {'dam': 5018, 'not_a_dam': 787}\n",
      "test_log {'dam': 6751, 'not_a_dam': 1043}\n",
      "southaf_log {'dam': 4563, 'not_a_dam': 100}\n",
      "\n",
      "\n",
      "2019-07-29 05:14:16,892 (2202603) INFO root [<module>:33] tf_record_iteration 43\n",
      "Starting to make TFRecords 43\n",
      "training_log {'dam': 21967, 'not_a_dam': 3387}\n",
      "validation_log {'dam': 5114, 'not_a_dam': 804}\n",
      "test_log {'dam': 6897, 'not_a_dam': 1067}\n",
      "southaf_log {'dam': 4661, 'not_a_dam': 103}\n",
      "\n",
      "\n",
      "2019-07-29 05:15:03,583 (2249294) INFO root [<module>:33] tf_record_iteration 44\n",
      "Starting to make TFRecords 44\n",
      "training_log {'dam': 22452, 'not_a_dam': 3453}\n",
      "validation_log {'dam': 5227, 'not_a_dam': 821}\n",
      "test_log {'dam': 7068, 'not_a_dam': 1094}\n",
      "southaf_log {'dam': 4780, 'not_a_dam': 105}\n",
      "\n",
      "\n",
      "2019-07-29 05:15:49,838 (2295549) INFO root [<module>:33] tf_record_iteration 45\n",
      "Starting to make TFRecords 45\n",
      "training_log {'dam': 22948, 'not_a_dam': 3527}\n",
      "validation_log {'dam': 5352, 'not_a_dam': 843}\n",
      "test_log {'dam': 7215, 'not_a_dam': 1123}\n",
      "southaf_log {'dam': 4886, 'not_a_dam': 106}\n",
      "\n",
      "\n",
      "2019-07-29 05:16:35,196 (2340907) INFO root [<module>:33] tf_record_iteration 46\n",
      "Starting to make TFRecords 46\n",
      "training_log {'dam': 23457, 'not_a_dam': 3594}\n",
      "validation_log {'dam': 5477, 'not_a_dam': 861}\n",
      "test_log {'dam': 7379, 'not_a_dam': 1147}\n",
      "southaf_log {'dam': 4977, 'not_a_dam': 108}\n",
      "\n",
      "\n",
      "2019-07-29 05:17:21,249 (2386959) INFO root [<module>:33] tf_record_iteration 47\n",
      "Starting to make TFRecords 47\n",
      "training_log {'dam': 23967, 'not_a_dam': 3664}\n",
      "validation_log {'dam': 5597, 'not_a_dam': 884}\n",
      "test_log {'dam': 7526, 'not_a_dam': 1167}\n",
      "southaf_log {'dam': 5085, 'not_a_dam': 110}\n",
      "\n",
      "\n",
      "2019-07-29 05:18:08,494 (2434205) INFO root [<module>:33] tf_record_iteration 48\n",
      "Starting to make TFRecords 48\n",
      "training_log {'dam': 24224, 'not_a_dam': 3712}\n",
      "validation_log {'dam': 5672, 'not_a_dam': 893}\n",
      "test_log {'dam': 7605, 'not_a_dam': 1180}\n",
      "southaf_log {'dam': 5155, 'not_a_dam': 114}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make workspace outputs directories doesn't exist\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'southaf_set')]\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Do the thing    \n",
    "training_log = {'dam': 0, 'not_a_dam': 0}\n",
    "validation_log = {'dam': 0, 'not_a_dam': 0}\n",
    "test_log = {'dam': 0, 'not_a_dam': 0}\n",
    "southaf_log = {'dam': 0, 'not_a_dam': 0}\n",
    "\n",
    "#last_time = time.time()\n",
    "\n",
    "training_writer_count = 0\n",
    "validation_writer_count = 0\n",
    "tf_record_iteration = 0\n",
    "max_tf_record_iteration = int(len(all_images_file_list)/Dams_per_round)\n",
    "\n",
    "\n",
    "\n",
    "while tf_record_iteration <= max_tf_record_iteration:\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        LOGGER.info('tf_record_iteration %d' % tf_record_iteration)\n",
    "\n",
    "        # Open writers\n",
    "        training_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'training_set/dams_%d.record' % tf_record_iteration))\n",
    "        validation_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'validation_set/dams_%d.record' % tf_record_iteration))\n",
    "        test_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'test_set/dams_%d.record' % tf_record_iteration))\n",
    "        southaf_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'southaf_set/dams_%d.record' % tf_record_iteration))\n",
    "\n",
    "        # Get a slice of the dataset\n",
    "        slice_dam_images_list = all_images_file_list[tf_record_iteration*Dams_per_round:min((tf_record_iteration+1)*Dams_per_round,len(all_images_file_list))]\n",
    "\n",
    "        #This is where I make TFRecords!\n",
    "        training_log, validation_log, test_log, southaf_log = make_TFRecords(slice_dam_images_list,tf_record_iteration)\n",
    "\n",
    "        # Close writers\n",
    "        training_writer.close()\n",
    "        validation_writer.close()\n",
    "        southaf_writer.close()\n",
    "        test_writer.close()\n",
    "\n",
    "        # Advance loop\n",
    "        tf_record_iteration += 1\n",
    "\n",
    "        print('training_log',training_log)\n",
    "        print('validation_log',validation_log)\n",
    "        print('test_log',test_log)\n",
    "        print('southaf_log',southaf_log)\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "#@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../../data/TFRecords_imagery_6-7_made_6-24',\n",
       " '../../../../data/TFRecords_imagery_6-7_made_6-24/training_set',\n",
       " '../../../../data/TFRecords_imagery_6-7_made_6-24/validation_set',\n",
       " '../../../../data/TFRecords_imagery_6-7_made_6-24/test_set',\n",
       " '../../../../data/TFRecords_imagery_6-7_made_6-24/southaf_set']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directories_to_make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 37 files...\n",
      "...Done\n",
      "Checking 37 files...\n",
      "...Done\n",
      "Checking 37 files...\n",
      "...Done\n",
      "Checking 37 files...\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Check files created\n",
    "\n",
    "for split_set in ['training_set','test_set','validation_set','southaf_set']:\n",
    "    files_to_check = glob.glob('../../../../data/TFRecords_imagery_6-7_made_6-21_cleaned/'+split_set+'/*')\n",
    "    print('Checking '+str(len(files_to_check))+' files...')\n",
    "\n",
    "    for filename in files_to_check:\n",
    "\n",
    "        try:\n",
    "            for example in tf.python_io.tf_record_iterator(filename): \n",
    "                tf_example = tf.train.Example.FromString(example) \n",
    "        except:\n",
    "            print(filename, ' is truncated')\n",
    "    print('...Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo mv TFRecords_imagery_6-7_made_6-24/southaf_set/dams_33.record TFRecords_imagery_6-7_made_6-21_cleaned/southaf_set/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-21 18:08:24,090 (6399624) INFO root [<module>:33] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {'dam': 536, 'not_a_dam': 107}\n",
      "validation_log {'dam': 120, 'not_a_dam': 25}\n",
      "test_log {'dam': 162, 'not_a_dam': 35}\n",
      "southaf_log {'dam': 13, 'not_a_dam': 2}\n",
      "\n",
      "\n",
      "2019-06-21 18:08:54,318 (6429852) INFO root [<module>:33] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {'dam': 1068, 'not_a_dam': 223}\n",
      "validation_log {'dam': 251, 'not_a_dam': 55}\n",
      "test_log {'dam': 316, 'not_a_dam': 61}\n",
      "southaf_log {'dam': 20, 'not_a_dam': 6}\n",
      "\n",
      "\n",
      "2019-06-21 18:09:25,313 (6460847) INFO root [<module>:33] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {'dam': 1616, 'not_a_dam': 321}\n",
      "validation_log {'dam': 378, 'not_a_dam': 75}\n",
      "test_log {'dam': 486, 'not_a_dam': 84}\n",
      "southaf_log {'dam': 30, 'not_a_dam': 10}\n",
      "\n",
      "\n",
      "2019-06-21 18:09:56,053 (6491587) INFO root [<module>:33] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {'dam': 2150, 'not_a_dam': 415}\n",
      "validation_log {'dam': 505, 'not_a_dam': 96}\n",
      "test_log {'dam': 656, 'not_a_dam': 123}\n",
      "southaf_log {'dam': 40, 'not_a_dam': 15}\n",
      "\n",
      "\n",
      "2019-06-21 18:10:27,655 (6523189) INFO root [<module>:33] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {'dam': 2660, 'not_a_dam': 508}\n",
      "validation_log {'dam': 652, 'not_a_dam': 124}\n",
      "test_log {'dam': 834, 'not_a_dam': 156}\n",
      "southaf_log {'dam': 47, 'not_a_dam': 19}\n",
      "\n",
      "\n",
      "2019-06-21 18:10:59,305 (6554839) INFO root [<module>:33] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {'dam': 3190, 'not_a_dam': 605}\n",
      "validation_log {'dam': 767, 'not_a_dam': 151}\n",
      "test_log {'dam': 1009, 'not_a_dam': 191}\n",
      "southaf_log {'dam': 62, 'not_a_dam': 25}\n",
      "\n",
      "\n",
      "2019-06-21 18:11:30,081 (6585615) INFO root [<module>:33] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {'dam': 3703, 'not_a_dam': 700}\n",
      "validation_log {'dam': 890, 'not_a_dam': 176}\n",
      "test_log {'dam': 1203, 'not_a_dam': 219}\n",
      "southaf_log {'dam': 83, 'not_a_dam': 26}\n",
      "\n",
      "\n",
      "2019-06-21 18:12:03,298 (6618832) INFO root [<module>:33] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {'dam': 4220, 'not_a_dam': 808}\n",
      "validation_log {'dam': 1013, 'not_a_dam': 200}\n",
      "test_log {'dam': 1377, 'not_a_dam': 258}\n",
      "southaf_log {'dam': 96, 'not_a_dam': 28}\n",
      "\n",
      "\n",
      "2019-06-21 18:12:40,435 (6655969) INFO root [<module>:33] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {'dam': 4769, 'not_a_dam': 903}\n",
      "validation_log {'dam': 1129, 'not_a_dam': 232}\n",
      "test_log {'dam': 1529, 'not_a_dam': 294}\n",
      "southaf_log {'dam': 114, 'not_a_dam': 30}\n",
      "\n",
      "\n",
      "2019-06-21 18:13:11,474 (6687008) INFO root [<module>:33] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {'dam': 5268, 'not_a_dam': 1011}\n",
      "validation_log {'dam': 1252, 'not_a_dam': 253}\n",
      "test_log {'dam': 1720, 'not_a_dam': 336}\n",
      "southaf_log {'dam': 127, 'not_a_dam': 33}\n",
      "\n",
      "\n",
      "2019-06-21 18:13:42,862 (6718396) INFO root [<module>:33] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {'dam': 5817, 'not_a_dam': 1110}\n",
      "validation_log {'dam': 1359, 'not_a_dam': 284}\n",
      "test_log {'dam': 1889, 'not_a_dam': 366}\n",
      "southaf_log {'dam': 140, 'not_a_dam': 35}\n",
      "\n",
      "\n",
      "2019-06-21 18:14:14,129 (6749663) INFO root [<module>:33] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n",
      "training_log {'dam': 6353, 'not_a_dam': 1216}\n",
      "validation_log {'dam': 1482, 'not_a_dam': 302}\n",
      "test_log {'dam': 2052, 'not_a_dam': 396}\n",
      "southaf_log {'dam': 162, 'not_a_dam': 37}\n",
      "\n",
      "\n",
      "2019-06-21 18:14:47,363 (6782897) INFO root [<module>:33] tf_record_iteration 12\n",
      "Starting to make TFRecords 12\n",
      "training_log {'dam': 6890, 'not_a_dam': 1310}\n",
      "validation_log {'dam': 1608, 'not_a_dam': 330}\n",
      "test_log {'dam': 2211, 'not_a_dam': 432}\n",
      "southaf_log {'dam': 178, 'not_a_dam': 41}\n",
      "\n",
      "\n",
      "2019-06-21 18:15:18,658 (6814192) INFO root [<module>:33] tf_record_iteration 13\n",
      "Starting to make TFRecords 13\n",
      "training_log {'dam': 7434, 'not_a_dam': 1414}\n",
      "validation_log {'dam': 1724, 'not_a_dam': 356}\n",
      "test_log {'dam': 2371, 'not_a_dam': 470}\n",
      "southaf_log {'dam': 187, 'not_a_dam': 44}\n",
      "\n",
      "\n",
      "2019-06-21 18:15:49,531 (6845065) INFO root [<module>:33] tf_record_iteration 14\n",
      "Starting to make TFRecords 14\n",
      "training_log {'dam': 7954, 'not_a_dam': 1527}\n",
      "validation_log {'dam': 1852, 'not_a_dam': 380}\n",
      "test_log {'dam': 2533, 'not_a_dam': 506}\n",
      "southaf_log {'dam': 199, 'not_a_dam': 49}\n",
      "\n",
      "\n",
      "2019-06-21 18:16:21,101 (6876635) INFO root [<module>:33] tf_record_iteration 15\n",
      "Starting to make TFRecords 15\n",
      "training_log {'dam': 8483, 'not_a_dam': 1629}\n",
      "validation_log {'dam': 1991, 'not_a_dam': 411}\n",
      "test_log {'dam': 2686, 'not_a_dam': 539}\n",
      "southaf_log {'dam': 205, 'not_a_dam': 56}\n",
      "\n",
      "\n",
      "2019-06-21 18:16:52,625 (6908159) INFO root [<module>:33] tf_record_iteration 16\n",
      "Starting to make TFRecords 16\n",
      "training_log {'dam': 9004, 'not_a_dam': 1738}\n",
      "validation_log {'dam': 2120, 'not_a_dam': 429}\n",
      "test_log {'dam': 2855, 'not_a_dam': 579}\n",
      "southaf_log {'dam': 217, 'not_a_dam': 58}\n",
      "\n",
      "\n",
      "2019-06-21 18:17:23,130 (6938664) INFO root [<module>:33] tf_record_iteration 17\n",
      "Starting to make TFRecords 17\n",
      "training_log {'dam': 9553, 'not_a_dam': 1841}\n",
      "validation_log {'dam': 2245, 'not_a_dam': 456}\n",
      "test_log {'dam': 3014, 'not_a_dam': 603}\n",
      "southaf_log {'dam': 227, 'not_a_dam': 61}\n",
      "\n",
      "\n",
      "2019-06-21 18:17:54,802 (6970336) INFO root [<module>:33] tf_record_iteration 18\n",
      "Starting to make TFRecords 18\n",
      "training_log {'dam': 10100, 'not_a_dam': 1951}\n",
      "validation_log {'dam': 2359, 'not_a_dam': 484}\n",
      "test_log {'dam': 3166, 'not_a_dam': 635}\n",
      "southaf_log {'dam': 244, 'not_a_dam': 61}\n",
      "\n",
      "\n",
      "2019-06-21 18:18:26,174 (7001708) INFO root [<module>:33] tf_record_iteration 19\n",
      "Starting to make TFRecords 19\n",
      "training_log {'dam': 10654, 'not_a_dam': 2046}\n",
      "validation_log {'dam': 2475, 'not_a_dam': 500}\n",
      "test_log {'dam': 3324, 'not_a_dam': 672}\n",
      "southaf_log {'dam': 264, 'not_a_dam': 65}\n",
      "\n",
      "\n",
      "2019-06-21 18:18:56,277 (7031811) INFO root [<module>:33] tf_record_iteration 20\n",
      "Starting to make TFRecords 20\n",
      "training_log {'dam': 11159, 'not_a_dam': 2147}\n",
      "validation_log {'dam': 2598, 'not_a_dam': 530}\n",
      "test_log {'dam': 3525, 'not_a_dam': 695}\n",
      "southaf_log {'dam': 278, 'not_a_dam': 68}\n",
      "\n",
      "\n",
      "2019-06-21 18:19:27,298 (7062832) INFO root [<module>:33] tf_record_iteration 21\n",
      "Starting to make TFRecords 21\n",
      "training_log {'dam': 11712, 'not_a_dam': 2265}\n",
      "validation_log {'dam': 2707, 'not_a_dam': 554}\n",
      "test_log {'dam': 3667, 'not_a_dam': 731}\n",
      "southaf_log {'dam': 292, 'not_a_dam': 72}\n",
      "\n",
      "\n",
      "2019-06-21 18:19:58,497 (7094031) INFO root [<module>:33] tf_record_iteration 22\n",
      "Starting to make TFRecords 22\n",
      "training_log {'dam': 12264, 'not_a_dam': 2366}\n",
      "validation_log {'dam': 2826, 'not_a_dam': 573}\n",
      "test_log {'dam': 3835, 'not_a_dam': 762}\n",
      "southaf_log {'dam': 300, 'not_a_dam': 74}\n",
      "\n",
      "\n",
      "2019-06-21 18:20:30,791 (7126324) INFO root [<module>:33] tf_record_iteration 23\n",
      "Starting to make TFRecords 23\n",
      "training_log {'dam': 12806, 'not_a_dam': 2474}\n",
      "validation_log {'dam': 2946, 'not_a_dam': 591}\n",
      "test_log {'dam': 4001, 'not_a_dam': 788}\n",
      "southaf_log {'dam': 314, 'not_a_dam': 80}\n",
      "\n",
      "\n",
      "2019-06-21 18:21:01,943 (7157477) INFO root [<module>:33] tf_record_iteration 24\n",
      "Starting to make TFRecords 24\n",
      "training_log {'dam': 13314, 'not_a_dam': 2589}\n",
      "validation_log {'dam': 3080, 'not_a_dam': 621}\n",
      "test_log {'dam': 4169, 'not_a_dam': 817}\n",
      "southaf_log {'dam': 329, 'not_a_dam': 81}\n",
      "\n",
      "\n",
      "2019-06-21 18:21:32,744 (7188278) INFO root [<module>:33] tf_record_iteration 25\n",
      "Starting to make TFRecords 25\n",
      "training_log {'dam': 13807, 'not_a_dam': 2706}\n",
      "validation_log {'dam': 3210, 'not_a_dam': 652}\n",
      "test_log {'dam': 4355, 'not_a_dam': 843}\n",
      "southaf_log {'dam': 342, 'not_a_dam': 85}\n",
      "\n",
      "\n",
      "2019-06-21 18:22:04,360 (7219894) INFO root [<module>:33] tf_record_iteration 26\n",
      "Starting to make TFRecords 26\n",
      "training_log {'dam': 14329, 'not_a_dam': 2806}\n",
      "validation_log {'dam': 3334, 'not_a_dam': 678}\n",
      "test_log {'dam': 4534, 'not_a_dam': 872}\n",
      "southaf_log {'dam': 357, 'not_a_dam': 90}\n",
      "\n",
      "\n",
      "2019-06-21 18:22:35,766 (7251300) INFO root [<module>:33] tf_record_iteration 27\n",
      "Starting to make TFRecords 27\n",
      "training_log {'dam': 14851, 'not_a_dam': 2921}\n",
      "validation_log {'dam': 3462, 'not_a_dam': 711}\n",
      "test_log {'dam': 4693, 'not_a_dam': 897}\n",
      "southaf_log {'dam': 370, 'not_a_dam': 95}\n",
      "\n",
      "\n",
      "2019-06-21 18:23:06,464 (7281998) INFO root [<module>:33] tf_record_iteration 28\n",
      "Starting to make TFRecords 28\n",
      "training_log {'dam': 15380, 'not_a_dam': 3013}\n",
      "validation_log {'dam': 3600, 'not_a_dam': 735}\n",
      "test_log {'dam': 4863, 'not_a_dam': 926}\n",
      "southaf_log {'dam': 387, 'not_a_dam': 96}\n",
      "\n",
      "\n",
      "2019-06-21 18:23:37,686 (7313220) INFO root [<module>:33] tf_record_iteration 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make TFRecords 29\n",
      "training_log {'dam': 15938, 'not_a_dam': 3110}\n",
      "validation_log {'dam': 3718, 'not_a_dam': 753}\n",
      "test_log {'dam': 5028, 'not_a_dam': 958}\n",
      "southaf_log {'dam': 398, 'not_a_dam': 97}\n",
      "\n",
      "\n",
      "2019-06-21 18:24:08,601 (7344135) INFO root [<module>:33] tf_record_iteration 30\n",
      "Starting to make TFRecords 30\n",
      "training_log {'dam': 16473, 'not_a_dam': 3198}\n",
      "validation_log {'dam': 3834, 'not_a_dam': 774}\n",
      "test_log {'dam': 5223, 'not_a_dam': 986}\n",
      "southaf_log {'dam': 411, 'not_a_dam': 101}\n",
      "\n",
      "\n",
      "2019-06-21 18:24:40,241 (7375775) INFO root [<module>:33] tf_record_iteration 31\n",
      "Starting to make TFRecords 31\n",
      "training_log {'dam': 17004, 'not_a_dam': 3287}\n",
      "validation_log {'dam': 3961, 'not_a_dam': 802}\n",
      "test_log {'dam': 5390, 'not_a_dam': 1020}\n",
      "southaf_log {'dam': 432, 'not_a_dam': 104}\n",
      "\n",
      "\n",
      "2019-06-21 18:25:11,637 (7407171) INFO root [<module>:33] tf_record_iteration 32\n",
      "Starting to make TFRecords 32\n",
      "training_log {'dam': 17533, 'not_a_dam': 3392}\n",
      "validation_log {'dam': 4081, 'not_a_dam': 827}\n",
      "test_log {'dam': 5571, 'not_a_dam': 1043}\n",
      "southaf_log {'dam': 446, 'not_a_dam': 107}\n",
      "\n",
      "\n",
      "2019-06-21 18:25:44,512 (7440046) INFO root [<module>:33] tf_record_iteration 33\n",
      "Starting to make TFRecords 33\n",
      "training_log {'dam': 18059, 'not_a_dam': 3500}\n",
      "validation_log {'dam': 4211, 'not_a_dam': 856}\n",
      "test_log {'dam': 5727, 'not_a_dam': 1079}\n",
      "southaf_log {'dam': 460, 'not_a_dam': 108}\n",
      "\n",
      "\n",
      "2019-06-21 18:26:16,213 (7471746) INFO root [<module>:33] tf_record_iteration 34\n",
      "Starting to make TFRecords 34\n",
      "training_log {'dam': 18624, 'not_a_dam': 3600}\n",
      "validation_log {'dam': 4349, 'not_a_dam': 882}\n",
      "test_log {'dam': 5853, 'not_a_dam': 1116}\n",
      "southaf_log {'dam': 466, 'not_a_dam': 110}\n",
      "\n",
      "\n",
      "2019-06-21 18:26:47,222 (7502756) INFO root [<module>:33] tf_record_iteration 35\n",
      "Starting to make TFRecords 35\n",
      "training_log {'dam': 19141, 'not_a_dam': 3697}\n",
      "validation_log {'dam': 4463, 'not_a_dam': 909}\n",
      "test_log {'dam': 6048, 'not_a_dam': 1140}\n",
      "southaf_log {'dam': 489, 'not_a_dam': 113}\n",
      "\n",
      "\n",
      "2019-06-21 18:27:17,698 (7533232) INFO root [<module>:33] tf_record_iteration 36\n",
      "Starting to make TFRecords 36\n",
      "training_log {'dam': 19264, 'not_a_dam': 3726}\n",
      "validation_log {'dam': 4496, 'not_a_dam': 914}\n",
      "test_log {'dam': 6087, 'not_a_dam': 1145}\n",
      "southaf_log {'dam': 490, 'not_a_dam': 114}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Log of TFRecords_imagery_6-7_made_6-21_ (dams_33.records truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-21 16:41:33,460 (1188994) INFO root [<module>:33] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {'dam': 818, 'not_a_dam': 167}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 13, 'not_a_dam': 2}\n",
      "\n",
      "\n",
      "2019-06-21 16:43:24,476 (1300010) INFO root [<module>:33] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {'dam': 1635, 'not_a_dam': 339}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 20, 'not_a_dam': 6}\n",
      "\n",
      "\n",
      "2019-06-21 16:45:09,947 (1405481) INFO root [<module>:33] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {'dam': 2480, 'not_a_dam': 480}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 30, 'not_a_dam': 10}\n",
      "\n",
      "\n",
      "2019-06-21 16:47:01,249 (1516783) INFO root [<module>:33] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {'dam': 3311, 'not_a_dam': 634}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 40, 'not_a_dam': 15}\n",
      "\n",
      "\n",
      "2019-06-21 16:48:44,967 (1620501) INFO root [<module>:33] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {'dam': 4146, 'not_a_dam': 788}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 47, 'not_a_dam': 19}\n",
      "\n",
      "\n",
      "2019-06-21 16:50:28,619 (1724153) INFO root [<module>:33] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {'dam': 4966, 'not_a_dam': 947}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 62, 'not_a_dam': 25}\n",
      "\n",
      "\n",
      "2019-06-21 16:52:12,618 (1828152) INFO root [<module>:33] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {'dam': 5796, 'not_a_dam': 1095}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 83, 'not_a_dam': 26}\n",
      "\n",
      "\n",
      "2019-06-21 16:53:58,053 (1933587) INFO root [<module>:33] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {'dam': 6610, 'not_a_dam': 1266}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 96, 'not_a_dam': 28}\n",
      "\n",
      "\n",
      "2019-06-21 16:55:43,662 (2039196) INFO root [<module>:33] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {'dam': 7427, 'not_a_dam': 1429}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 114, 'not_a_dam': 30}\n",
      "\n",
      "\n",
      "2019-06-21 16:57:30,300 (2145834) INFO root [<module>:33] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {'dam': 8240, 'not_a_dam': 1600}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 127, 'not_a_dam': 33}\n",
      "\n",
      "\n",
      "2019-06-21 16:59:15,319 (2250853) INFO root [<module>:33] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {'dam': 9065, 'not_a_dam': 1760}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 140, 'not_a_dam': 35}\n",
      "\n",
      "\n",
      "2019-06-21 17:00:56,920 (2352454) INFO root [<module>:33] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n",
      "training_log {'dam': 9887, 'not_a_dam': 1914}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 162, 'not_a_dam': 37}\n",
      "\n",
      "\n",
      "2019-06-21 17:02:42,994 (2458528) INFO root [<module>:33] tf_record_iteration 12\n",
      "Starting to make TFRecords 12\n",
      "training_log {'dam': 10709, 'not_a_dam': 2072}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 178, 'not_a_dam': 41}\n",
      "\n",
      "\n",
      "2019-06-21 17:04:30,541 (2566075) INFO root [<module>:33] tf_record_iteration 13\n",
      "Starting to make TFRecords 13\n",
      "training_log {'dam': 11529, 'not_a_dam': 2240}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 187, 'not_a_dam': 44}\n",
      "\n",
      "\n",
      "2019-06-21 17:06:17,503 (2673037) INFO root [<module>:33] tf_record_iteration 14\n",
      "Starting to make TFRecords 14\n",
      "training_log {'dam': 12339, 'not_a_dam': 2413}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 199, 'not_a_dam': 49}\n",
      "\n",
      "\n",
      "2019-06-21 17:08:02,977 (2778511) INFO root [<module>:33] tf_record_iteration 15\n",
      "Starting to make TFRecords 15\n",
      "training_log {'dam': 13160, 'not_a_dam': 2579}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 205, 'not_a_dam': 56}\n",
      "\n",
      "\n",
      "2019-06-21 17:09:51,207 (2886741) INFO root [<module>:33] tf_record_iteration 16\n",
      "Starting to make TFRecords 16\n",
      "training_log {'dam': 13979, 'not_a_dam': 2746}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 217, 'not_a_dam': 58}\n",
      "\n",
      "\n",
      "2019-06-21 17:11:41,617 (2997150) INFO root [<module>:33] tf_record_iteration 17\n",
      "Starting to make TFRecords 17\n",
      "training_log {'dam': 14812, 'not_a_dam': 2900}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 227, 'not_a_dam': 61}\n",
      "\n",
      "\n",
      "2019-06-21 17:13:25,765 (3101299) INFO root [<module>:33] tf_record_iteration 18\n",
      "Starting to make TFRecords 18\n",
      "training_log {'dam': 15625, 'not_a_dam': 3070}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 244, 'not_a_dam': 61}\n",
      "\n",
      "\n",
      "2019-06-21 17:15:11,202 (3206736) INFO root [<module>:33] tf_record_iteration 19\n",
      "Starting to make TFRecords 19\n",
      "training_log {'dam': 16453, 'not_a_dam': 3218}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 264, 'not_a_dam': 65}\n",
      "\n",
      "\n",
      "2019-06-21 17:16:55,199 (3310733) INFO root [<module>:33] tf_record_iteration 20\n",
      "Starting to make TFRecords 20\n",
      "training_log {'dam': 17282, 'not_a_dam': 3372}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 278, 'not_a_dam': 68}\n",
      "\n",
      "\n",
      "2019-06-21 17:18:41,585 (3417119) INFO root [<module>:33] tf_record_iteration 21\n",
      "Starting to make TFRecords 21\n",
      "training_log {'dam': 18086, 'not_a_dam': 3550}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 292, 'not_a_dam': 72}\n",
      "\n",
      "\n",
      "2019-06-21 17:20:26,413 (3521947) INFO root [<module>:33] tf_record_iteration 22\n",
      "Starting to make TFRecords 22\n",
      "training_log {'dam': 18925, 'not_a_dam': 3701}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 300, 'not_a_dam': 74}\n",
      "\n",
      "\n",
      "2019-06-21 17:22:11,514 (3627048) INFO root [<module>:33] tf_record_iteration 23\n",
      "Starting to make TFRecords 23\n",
      "training_log {'dam': 19753, 'not_a_dam': 3853}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 314, 'not_a_dam': 80}\n",
      "\n",
      "\n",
      "2019-06-21 17:24:05,014 (3740548) INFO root [<module>:33] tf_record_iteration 24\n",
      "Starting to make TFRecords 24\n",
      "training_log {'dam': 20563, 'not_a_dam': 4027}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 329, 'not_a_dam': 81}\n",
      "\n",
      "\n",
      "2019-06-21 17:25:51,139 (3846673) INFO root [<module>:33] tf_record_iteration 25\n",
      "Starting to make TFRecords 25\n",
      "training_log {'dam': 21372, 'not_a_dam': 4201}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 342, 'not_a_dam': 85}\n",
      "\n",
      "\n",
      "2019-06-21 17:27:39,056 (3954590) INFO root [<module>:33] tf_record_iteration 26\n",
      "Starting to make TFRecords 26\n",
      "training_log {'dam': 22197, 'not_a_dam': 4356}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 357, 'not_a_dam': 90}\n",
      "\n",
      "\n",
      "2019-06-21 17:29:24,450 (4059984) INFO root [<module>:33] tf_record_iteration 27\n",
      "Starting to make TFRecords 27\n",
      "training_log {'dam': 23006, 'not_a_dam': 4529}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 370, 'not_a_dam': 95}\n",
      "\n",
      "\n",
      "2019-06-21 17:31:12,400 (4167934) INFO root [<module>:33] tf_record_iteration 28\n",
      "Starting to make TFRecords 28\n",
      "training_log {'dam': 23843, 'not_a_dam': 4674}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 387, 'not_a_dam': 96}\n",
      "\n",
      "\n",
      "2019-06-21 17:32:58,953 (4274487) INFO root [<module>:33] tf_record_iteration 29\n",
      "Starting to make TFRecords 29\n",
      "training_log {'dam': 24684, 'not_a_dam': 4821}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 398, 'not_a_dam': 97}\n",
      "\n",
      "\n",
      "2019-06-21 17:34:41,420 (4376954) INFO root [<module>:33] tf_record_iteration 30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make TFRecords 30\n",
      "training_log {'dam': 25530, 'not_a_dam': 4958}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 411, 'not_a_dam': 101}\n",
      "\n",
      "\n",
      "2019-06-21 17:36:26,042 (4481576) INFO root [<module>:33] tf_record_iteration 31\n",
      "Starting to make TFRecords 31\n",
      "training_log {'dam': 26355, 'not_a_dam': 5109}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 432, 'not_a_dam': 104}\n",
      "\n",
      "\n",
      "2019-06-21 17:38:10,935 (4586469) INFO root [<module>:33] tf_record_iteration 32\n",
      "Starting to make TFRecords 32\n",
      "training_log {'dam': 27185, 'not_a_dam': 5262}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 446, 'not_a_dam': 107}\n",
      "\n",
      "\n",
      "2019-06-21 17:39:59,350 (4694884) INFO root [<module>:33] tf_record_iteration 33\n",
      "Starting to make TFRecords 33\n",
      "training_log {'dam': 27997, 'not_a_dam': 5435}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 460, 'not_a_dam': 108}\n",
      "\n",
      "\n",
      "2019-06-21 17:41:45,403 (4800937) INFO root [<module>:33] tf_record_iteration 34\n",
      "Starting to make TFRecords 34\n",
      "training_log {'dam': 28826, 'not_a_dam': 5598}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 466, 'not_a_dam': 110}\n",
      "\n",
      "\n",
      "2019-06-21 17:43:34,429 (4909963) INFO root [<module>:33] tf_record_iteration 35\n",
      "Starting to make TFRecords 35\n",
      "training_log {'dam': 29652, 'not_a_dam': 5746}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 489, 'not_a_dam': 113}\n",
      "\n",
      "\n",
      "2019-06-21 17:45:22,937 (5018471) INFO root [<module>:33] tf_record_iteration 36\n",
      "Starting to make TFRecords 36\n",
      "training_log {'dam': 29847, 'not_a_dam': 5785}\n",
      "validation_log {'dam': 0, 'not_a_dam': 0}\n",
      "test_log {'dam': 0, 'not_a_dam': 0}\n",
      "southaf_log {'dam': 490, 'not_a_dam': 114}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Log of TFRecords_imagery_6-7_made_6-21 (random number constant!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-15 00:14:46,751 (70525) INFO root [<module>:33] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {b'not_a_dam': 90, b'dam': 530}\n",
      "validation_log {b'not_a_dam': 24, b'dam': 133}\n",
      "test_log {b'not_a_dam': 31, b'dam': 167}\n",
      "southaf_log {b'not_a_dam': 6, b'dam': 19}\n",
      "\n",
      "\n",
      "2019-06-15 00:15:18,382 (102156) INFO root [<module>:33] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {b'not_a_dam': 171, b'dam': 1091}\n",
      "validation_log {b'not_a_dam': 37, b'dam': 260}\n",
      "test_log {b'not_a_dam': 68, b'dam': 333}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 33}\n",
      "\n",
      "\n",
      "2019-06-15 00:15:53,226 (137000) INFO root [<module>:33] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {b'not_a_dam': 272, b'dam': 1634}\n",
      "validation_log {b'not_a_dam': 63, b'dam': 371}\n",
      "test_log {b'not_a_dam': 94, b'dam': 498}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 56}\n",
      "\n",
      "\n",
      "2019-06-15 00:16:30,640 (174414) INFO root [<module>:33] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {b'not_a_dam': 370, b'dam': 2186}\n",
      "validation_log {b'not_a_dam': 81, b'dam': 500}\n",
      "test_log {b'not_a_dam': 129, b'dam': 653}\n",
      "southaf_log {b'not_a_dam': 13, b'dam': 68}\n",
      "\n",
      "\n",
      "2019-06-15 00:17:07,535 (211309) INFO root [<module>:33] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {b'not_a_dam': 461, b'dam': 2760}\n",
      "validation_log {b'not_a_dam': 91, b'dam': 625}\n",
      "test_log {b'not_a_dam': 160, b'dam': 812}\n",
      "southaf_log {b'not_a_dam': 15, b'dam': 76}\n",
      "\n",
      "\n",
      "2019-06-15 00:17:41,627 (245401) INFO root [<module>:33] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {b'not_a_dam': 553, b'dam': 3315}\n",
      "validation_log {b'not_a_dam': 107, b'dam': 753}\n",
      "test_log {b'not_a_dam': 184, b'dam': 983}\n",
      "southaf_log {b'not_a_dam': 17, b'dam': 88}\n",
      "\n",
      "\n",
      "2019-06-15 00:18:19,920 (283694) INFO root [<module>:33] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {b'not_a_dam': 644, b'dam': 3856}\n",
      "validation_log {b'not_a_dam': 127, b'dam': 861}\n",
      "test_log {b'not_a_dam': 216, b'dam': 1181}\n",
      "southaf_log {b'not_a_dam': 18, b'dam': 97}\n",
      "\n",
      "\n",
      "2019-06-15 00:18:57,134 (320908) INFO root [<module>:33] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {b'not_a_dam': 709, b'dam': 4413}\n",
      "validation_log {b'not_a_dam': 146, b'dam': 1000}\n",
      "test_log {b'not_a_dam': 244, b'dam': 1352}\n",
      "southaf_log {b'not_a_dam': 25, b'dam': 111}\n",
      "\n",
      "\n",
      "2019-06-15 00:19:30,152 (353926) INFO root [<module>:33] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {b'not_a_dam': 801, b'dam': 4948}\n",
      "validation_log {b'not_a_dam': 167, b'dam': 1142}\n",
      "test_log {b'not_a_dam': 274, b'dam': 1518}\n",
      "southaf_log {b'not_a_dam': 25, b'dam': 125}\n",
      "\n",
      "\n",
      "2019-06-15 00:20:06,614 (390388) INFO root [<module>:33] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {b'not_a_dam': 888, b'dam': 5496}\n",
      "validation_log {b'not_a_dam': 189, b'dam': 1281}\n",
      "test_log {b'not_a_dam': 301, b'dam': 1684}\n",
      "southaf_log {b'not_a_dam': 26, b'dam': 135}\n",
      "\n",
      "\n",
      "2019-06-15 00:21:12,907 (456681) INFO root [<module>:33] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {b'not_a_dam': 981, b'dam': 6029}\n",
      "validation_log {b'not_a_dam': 223, b'dam': 1393}\n",
      "test_log {b'not_a_dam': 323, b'dam': 1865}\n",
      "southaf_log {b'not_a_dam': 28, b'dam': 158}\n",
      "\n",
      "\n",
      "2019-06-15 00:22:16,352 (520126) INFO root [<module>:33] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n",
      "training_log {b'not_a_dam': 1062, b'dam': 6600}\n",
      "validation_log {b'not_a_dam': 241, b'dam': 1507}\n",
      "test_log {b'not_a_dam': 353, b'dam': 2035}\n",
      "southaf_log {b'not_a_dam': 32, b'dam': 170}\n",
      "\n",
      "\n",
      "2019-06-15 00:23:33,201 (596975) INFO root [<module>:33] tf_record_iteration 12\n",
      "Starting to make TFRecords 12\n",
      "training_log {b'not_a_dam': 1147, b'dam': 7146}\n",
      "validation_log {b'not_a_dam': 261, b'dam': 1622}\n",
      "test_log {b'not_a_dam': 382, b'dam': 2224}\n",
      "southaf_log {b'not_a_dam': 34, b'dam': 184}\n",
      "\n",
      "\n",
      "2019-06-15 00:24:48,622 (672396) INFO root [<module>:33] tf_record_iteration 13\n",
      "Starting to make TFRecords 13\n",
      "training_log {b'not_a_dam': 1226, b'dam': 7674}\n",
      "validation_log {b'not_a_dam': 287, b'dam': 1762}\n",
      "test_log {b'not_a_dam': 414, b'dam': 2401}\n",
      "southaf_log {b'not_a_dam': 35, b'dam': 201}\n",
      "\n",
      "\n",
      "2019-06-15 00:26:03,074 (746848) INFO root [<module>:33] tf_record_iteration 14\n",
      "Starting to make TFRecords 14\n",
      "training_log {b'not_a_dam': 1317, b'dam': 8218}\n",
      "validation_log {b'not_a_dam': 309, b'dam': 1890}\n",
      "test_log {b'not_a_dam': 448, b'dam': 2567}\n",
      "southaf_log {b'not_a_dam': 40, b'dam': 211}\n",
      "\n",
      "\n",
      "2019-06-15 00:27:19,124 (822898) INFO root [<module>:33] tf_record_iteration 15\n",
      "Starting to make TFRecords 15\n",
      "training_log {b'not_a_dam': 1407, b'dam': 8778}\n",
      "validation_log {b'not_a_dam': 328, b'dam': 2018}\n",
      "test_log {b'not_a_dam': 477, b'dam': 2724}\n",
      "southaf_log {b'not_a_dam': 42, b'dam': 226}\n",
      "\n",
      "\n",
      "2019-06-15 00:28:34,750 (898524) INFO root [<module>:33] tf_record_iteration 16\n",
      "Starting to make TFRecords 16\n",
      "training_log {b'not_a_dam': 1493, b'dam': 9323}\n",
      "validation_log {b'not_a_dam': 352, b'dam': 2141}\n",
      "test_log {b'not_a_dam': 504, b'dam': 2896}\n",
      "southaf_log {b'not_a_dam': 48, b'dam': 243}\n",
      "\n",
      "\n",
      "2019-06-15 00:29:52,583 (976357) INFO root [<module>:33] tf_record_iteration 17\n",
      "Starting to make TFRecords 17\n",
      "training_log {b'not_a_dam': 1576, b'dam': 9883}\n",
      "validation_log {b'not_a_dam': 370, b'dam': 2283}\n",
      "test_log {b'not_a_dam': 533, b'dam': 3051}\n",
      "southaf_log {b'not_a_dam': 49, b'dam': 255}\n",
      "\n",
      "\n",
      "2019-06-15 00:31:13,022 (1056796) INFO root [<module>:33] tf_record_iteration 18\n",
      "Starting to make TFRecords 18\n",
      "training_log {b'not_a_dam': 1686, b'dam': 10426}\n",
      "validation_log {b'not_a_dam': 388, b'dam': 2412}\n",
      "test_log {b'not_a_dam': 559, b'dam': 3210}\n",
      "southaf_log {b'not_a_dam': 52, b'dam': 267}\n",
      "\n",
      "\n",
      "2019-06-15 00:32:32,754 (1136528) INFO root [<module>:33] tf_record_iteration 19\n",
      "Starting to make TFRecords 19\n",
      "training_log {b'not_a_dam': 1769, b'dam': 10985}\n",
      "validation_log {b'not_a_dam': 410, b'dam': 2548}\n",
      "test_log {b'not_a_dam': 569, b'dam': 3380}\n",
      "southaf_log {b'not_a_dam': 53, b'dam': 286}\n",
      "\n",
      "\n",
      "2019-06-15 00:33:57,080 (1220854) INFO root [<module>:33] tf_record_iteration 20\n",
      "Starting to make TFRecords 20\n",
      "training_log {b'not_a_dam': 1868, b'dam': 11541}\n",
      "validation_log {b'not_a_dam': 424, b'dam': 2673}\n",
      "test_log {b'not_a_dam': 593, b'dam': 3546}\n",
      "southaf_log {b'not_a_dam': 56, b'dam': 299}\n",
      "\n",
      "\n",
      "2019-06-15 00:35:20,509 (1304283) INFO root [<module>:33] tf_record_iteration 21\n",
      "Starting to make TFRecords 21\n",
      "training_log {b'not_a_dam': 1961, b'dam': 12091}\n",
      "validation_log {b'not_a_dam': 440, b'dam': 2803}\n",
      "test_log {b'not_a_dam': 618, b'dam': 3713}\n",
      "southaf_log {b'not_a_dam': 58, b'dam': 316}\n",
      "\n",
      "\n",
      "2019-06-15 00:36:43,994 (1387768) INFO root [<module>:33] tf_record_iteration 22\n",
      "Starting to make TFRecords 22\n",
      "training_log {b'not_a_dam': 2051, b'dam': 12641}\n",
      "validation_log {b'not_a_dam': 458, b'dam': 2918}\n",
      "test_log {b'not_a_dam': 640, b'dam': 3904}\n",
      "southaf_log {b'not_a_dam': 63, b'dam': 325}\n",
      "\n",
      "\n",
      "2019-06-15 00:38:06,996 (1470770) INFO root [<module>:33] tf_record_iteration 23\n",
      "Starting to make TFRecords 23\n",
      "training_log {b'not_a_dam': 2157, b'dam': 13173}\n",
      "validation_log {b'not_a_dam': 478, b'dam': 3038}\n",
      "test_log {b'not_a_dam': 672, b'dam': 4075}\n",
      "southaf_log {b'not_a_dam': 66, b'dam': 341}\n",
      "\n",
      "\n",
      "2019-06-15 00:39:28,954 (1552728) INFO root [<module>:33] tf_record_iteration 24\n",
      "Starting to make TFRecords 24\n",
      "training_log {b'not_a_dam': 2245, b'dam': 13757}\n",
      "validation_log {b'not_a_dam': 491, b'dam': 3139}\n",
      "test_log {b'not_a_dam': 695, b'dam': 4250}\n",
      "southaf_log {b'not_a_dam': 68, b'dam': 355}\n",
      "\n",
      "\n",
      "2019-06-15 00:40:53,985 (1637759) INFO root [<module>:33] tf_record_iteration 25\n",
      "Starting to make TFRecords 25\n",
      "training_log {b'not_a_dam': 2339, b'dam': 14321}\n",
      "validation_log {b'not_a_dam': 507, b'dam': 3262}\n",
      "test_log {b'not_a_dam': 724, b'dam': 4402}\n",
      "southaf_log {b'not_a_dam': 70, b'dam': 375}\n",
      "\n",
      "\n",
      "2019-06-15 00:42:17,861 (1721635) INFO root [<module>:33] tf_record_iteration 26\n",
      "Starting to make TFRecords 26\n",
      "training_log {b'not_a_dam': 2420, b'dam': 14891}\n",
      "validation_log {b'not_a_dam': 529, b'dam': 3395}\n",
      "test_log {b'not_a_dam': 745, b'dam': 4560}\n",
      "southaf_log {b'not_a_dam': 70, b'dam': 390}\n",
      "\n",
      "\n",
      "2019-06-15 00:43:42,949 (1806723) INFO root [<module>:33] tf_record_iteration 27\n",
      "Starting to make TFRecords 27\n",
      "training_log {b'not_a_dam': 2519, b'dam': 15426}\n",
      "validation_log {b'not_a_dam': 547, b'dam': 3530}\n",
      "test_log {b'not_a_dam': 774, b'dam': 4725}\n",
      "southaf_log {b'not_a_dam': 73, b'dam': 406}\n",
      "\n",
      "\n",
      "2019-06-15 00:45:10,886 (1894660) INFO root [<module>:33] tf_record_iteration 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make TFRecords 28\n",
      "training_log {b'not_a_dam': 2610, b'dam': 15964}\n",
      "validation_log {b'not_a_dam': 576, b'dam': 3643}\n",
      "test_log {b'not_a_dam': 809, b'dam': 4897}\n",
      "southaf_log {b'not_a_dam': 76, b'dam': 425}\n",
      "\n",
      "\n",
      "2019-06-15 00:46:36,437 (1980211) INFO root [<module>:33] tf_record_iteration 29\n",
      "Starting to make TFRecords 29\n",
      "training_log {b'not_a_dam': 2692, b'dam': 16518}\n",
      "validation_log {b'not_a_dam': 602, b'dam': 3785}\n",
      "test_log {b'not_a_dam': 848, b'dam': 5037}\n",
      "southaf_log {b'not_a_dam': 80, b'dam': 438}\n",
      "\n",
      "\n",
      "2019-06-15 00:48:03,309 (2067083) INFO root [<module>:33] tf_record_iteration 30\n",
      "Starting to make TFRecords 30\n",
      "training_log {b'not_a_dam': 2799, b'dam': 17032}\n",
      "validation_log {b'not_a_dam': 620, b'dam': 3940}\n",
      "test_log {b'not_a_dam': 876, b'dam': 5200}\n",
      "southaf_log {b'not_a_dam': 82, b'dam': 451}\n",
      "\n",
      "\n",
      "2019-06-15 00:49:31,558 (2155332) INFO root [<module>:33] tf_record_iteration 31\n",
      "Starting to make TFRecords 31\n",
      "training_log {b'not_a_dam': 2879, b'dam': 17594}\n",
      "validation_log {b'not_a_dam': 643, b'dam': 4076}\n",
      "test_log {b'not_a_dam': 899, b'dam': 5360}\n",
      "southaf_log {b'not_a_dam': 84, b'dam': 465}\n",
      "\n",
      "\n",
      "2019-06-15 00:51:03,081 (2246855) INFO root [<module>:33] tf_record_iteration 32\n",
      "Starting to make TFRecords 32\n",
      "training_log {b'not_a_dam': 2956, b'dam': 18150}\n",
      "validation_log {b'not_a_dam': 663, b'dam': 4202}\n",
      "test_log {b'not_a_dam': 935, b'dam': 5531}\n",
      "southaf_log {b'not_a_dam': 88, b'dam': 475}\n",
      "\n",
      "\n",
      "2019-06-15 00:52:32,126 (2335899) INFO root [<module>:33] tf_record_iteration 33\n",
      "Starting to make TFRecords 33\n",
      "training_log {b'not_a_dam': 3054, b'dam': 18685}\n",
      "validation_log {b'not_a_dam': 681, b'dam': 4325}\n",
      "test_log {b'not_a_dam': 962, b'dam': 5714}\n",
      "southaf_log {b'not_a_dam': 89, b'dam': 490}\n",
      "\n",
      "\n",
      "2019-06-15 00:54:05,871 (2429645) INFO root [<module>:33] tf_record_iteration 34\n",
      "Starting to make TFRecords 34\n",
      "training_log {b'not_a_dam': 3145, b'dam': 19237}\n",
      "validation_log {b'not_a_dam': 703, b'dam': 4463}\n",
      "test_log {b'not_a_dam': 989, b'dam': 5869}\n",
      "southaf_log {b'not_a_dam': 94, b'dam': 500}\n",
      "\n",
      "\n",
      "2019-06-15 00:55:33,827 (2517601) INFO root [<module>:33] tf_record_iteration 35\n",
      "Starting to make TFRecords 35\n",
      "training_log {b'not_a_dam': 3223, b'dam': 19817}\n",
      "validation_log {b'not_a_dam': 723, b'dam': 4580}\n",
      "test_log {b'not_a_dam': 1016, b'dam': 6036}\n",
      "southaf_log {b'not_a_dam': 95, b'dam': 510}\n",
      "\n",
      "\n",
      "2019-06-15 00:57:05,598 (2609372) INFO root [<module>:33] tf_record_iteration 36\n",
      "Starting to make TFRecords 36\n",
      "training_log {b'not_a_dam': 3312, b'dam': 20386}\n",
      "validation_log {b'not_a_dam': 741, b'dam': 4691}\n",
      "test_log {b'not_a_dam': 1049, b'dam': 6202}\n",
      "southaf_log {b'not_a_dam': 98, b'dam': 521}\n",
      "\n",
      "\n",
      "2019-06-15 00:58:35,130 (2698904) INFO root [<module>:33] tf_record_iteration 37\n",
      "Starting to make TFRecords 37\n",
      "training_log {b'not_a_dam': 3403, b'dam': 20944}\n",
      "validation_log {b'not_a_dam': 760, b'dam': 4831}\n",
      "test_log {b'not_a_dam': 1078, b'dam': 6351}\n",
      "southaf_log {b'not_a_dam': 101, b'dam': 532}\n",
      "\n",
      "\n",
      "2019-06-15 01:00:03,780 (2787554) INFO root [<module>:33] tf_record_iteration 38\n",
      "Starting to make TFRecords 38\n",
      "training_log {b'not_a_dam': 3491, b'dam': 21510}\n",
      "validation_log {b'not_a_dam': 777, b'dam': 4945}\n",
      "test_log {b'not_a_dam': 1110, b'dam': 6507}\n",
      "southaf_log {b'not_a_dam': 108, b'dam': 552}\n",
      "\n",
      "\n",
      "2019-06-15 01:01:34,438 (2878212) INFO root [<module>:33] tf_record_iteration 39\n",
      "Starting to make TFRecords 39\n",
      "training_log {b'not_a_dam': 3581, b'dam': 22054}\n",
      "validation_log {b'not_a_dam': 800, b'dam': 5077}\n",
      "test_log {b'not_a_dam': 1135, b'dam': 6671}\n",
      "southaf_log {b'not_a_dam': 109, b'dam': 573}\n",
      "\n",
      "\n",
      "2019-06-15 01:03:04,900 (2968674) INFO root [<module>:33] tf_record_iteration 40\n",
      "Starting to make TFRecords 40\n",
      "training_log {b'not_a_dam': 3666, b'dam': 22617}\n",
      "validation_log {b'not_a_dam': 814, b'dam': 5212}\n",
      "test_log {b'not_a_dam': 1161, b'dam': 6836}\n",
      "southaf_log {b'not_a_dam': 109, b'dam': 585}\n",
      "\n",
      "\n",
      "2019-06-15 01:04:35,297 (3059071) INFO root [<module>:33] tf_record_iteration 41\n",
      "Starting to make TFRecords 41\n",
      "training_log {b'not_a_dam': 3751, b'dam': 23165}\n",
      "validation_log {b'not_a_dam': 831, b'dam': 5356}\n",
      "test_log {b'not_a_dam': 1188, b'dam': 6994}\n",
      "southaf_log {b'not_a_dam': 114, b'dam': 601}\n",
      "\n",
      "\n",
      "2019-06-15 01:06:06,528 (3150302) INFO root [<module>:33] tf_record_iteration 42\n",
      "Starting to make TFRecords 42\n",
      "training_log {b'not_a_dam': 3759, b'dam': 23236}\n",
      "validation_log {b'not_a_dam': 834, b'dam': 5377}\n",
      "test_log {b'not_a_dam': 1192, b'dam': 7019}\n",
      "southaf_log {b'not_a_dam': 114, b'dam': 604}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For reference -- output of the full Make_TFRecords loop for \n",
    "# TFRecords_imagery_6-7_made_6-14.\n",
    "# (Mistake coutning not_a_dams twice as dams then not_a_dam.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel available now\n"
     ]
    }
   ],
   "source": [
    "print('Kernel available now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes & stuffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making TFRecords just for the not_a_dam_8-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_charlie_root = \"../../../..\"\n",
    "\n",
    "OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/TFRecords_not_a_dam_08-26\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Must re-crop all images to 418* 418 -_- (Maanas has said they are 419 - but they are 418!!!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def crop_419_to_418(image):\n",
    "    width, height = image.size\n",
    "    left = 0\n",
    "    top = 0\n",
    "    right = width-1\n",
    "    bottom = height-1\n",
    "    cropped = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped 100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 1900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 2900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 3900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 4900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 5900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 6900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 7900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 8900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 9900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 10900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 11900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12100 images in ../../../../data/not_a_dam_08-26/cropped_418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped 12200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 12900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 13900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 14900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 15900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 16900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 17900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 18900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 19900 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20000 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20100 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20200 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20300 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20400 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20500 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20600 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20700 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20800 images in ../../../../data/not_a_dam_08-26/cropped_418\n",
      "Cropped 20900 images in ../../../../data/not_a_dam_08-26/cropped_418\n"
     ]
    }
   ],
   "source": [
    "IMG_DIR_419 = os.path.join(path_to_charlie_root,\"data/not_a_dam_08-26/autogeneratedimages\")\n",
    "IMG_DIR_418 = os.path.join(path_to_charlie_root,\"data/not_a_dam_08-26/cropped_418\")\n",
    "\n",
    "k =0\n",
    "for f in os.listdir(IMG_DIR_419):\n",
    "    k+=1\n",
    "    \n",
    "    image = Image.open(os.path.join(IMG_DIR_419,f))   \n",
    "    \n",
    "    try:\n",
    "        cropped_image = crop_419_to_418(image)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    cropped_image.save(os.path.join(IMG_DIR_418,f))\n",
    "    \n",
    "    if k%100==0:\n",
    "        print('Cropped '+str(k)+' images in '+IMG_DIR_418)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOT_A_DAM_IMAGE_DIR = IMG_DIR_418\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "    \n",
    "\n",
    "## Full dataset \n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20915"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_portion = .2\n",
    "Dams_per_round = 1000 # = max_dams_per_record \n",
    "\n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now make TFRecords - MODIFIED FUNCTION SO THEY ONLY GO IN TRAIN+VAL SETS\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords(dam_file_list,tf_record_iteration):\n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "    random.seed(tf_record_iteration)\n",
    "    random_numbers_list = [random.random() for x in range(0, len(dam_file_list))]\n",
    "    random_number_iterator = 0\n",
    "    \n",
    "    for image_path in dam_file_list:\n",
    "\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "\n",
    "        # if this image is a dam:\n",
    "\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if not 'not_a_dam' in image_path:\n",
    "            dam_type = 'dam'\n",
    "            print('THIS SHOULD NOT HAPPEN WITH THIS SET')\n",
    "\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax > 1 or ymax > 1):\n",
    "                #LOGGER.warning('bounding box out of bounds %s %s %s %s',\n",
    "                #               xmin, xmax, ymin, ymax)\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = 'not_a_dam'\n",
    "\n",
    "            \n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "\n",
    "        \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "        #try:\n",
    "        #    centroid = image_metadata['lng_lat_centroid']\n",
    "        #except NameError:\n",
    "        #    raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "            \n",
    "            \n",
    "        random_number = random_numbers_list[random_number_iterator]\n",
    "        random_number_iterator+=1\n",
    "        \n",
    "        if random_number > (1-validation_set_portion):\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return training_log, validation_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-26 17:55:31,011 (5648226) INFO root [<module>:35] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {'not_a_dam': 810, 'dam': 0}\n",
      "validation_log {'not_a_dam': 190, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:56:02,293 (5679508) INFO root [<module>:35] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {'not_a_dam': 1592, 'dam': 0}\n",
      "validation_log {'not_a_dam': 408, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:56:34,721 (5711935) INFO root [<module>:35] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {'not_a_dam': 2392, 'dam': 0}\n",
      "validation_log {'not_a_dam': 608, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:57:06,474 (5743689) INFO root [<module>:35] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {'not_a_dam': 3187, 'dam': 0}\n",
      "validation_log {'not_a_dam': 813, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:57:39,689 (5776903) INFO root [<module>:35] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {'not_a_dam': 3956, 'dam': 0}\n",
      "validation_log {'not_a_dam': 1044, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:58:12,491 (5809706) INFO root [<module>:35] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {'not_a_dam': 4768, 'dam': 0}\n",
      "validation_log {'not_a_dam': 1232, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:58:46,514 (5843728) INFO root [<module>:35] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {'not_a_dam': 5567, 'dam': 0}\n",
      "validation_log {'not_a_dam': 1433, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:59:20,841 (5878056) INFO root [<module>:35] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {'not_a_dam': 6364, 'dam': 0}\n",
      "validation_log {'not_a_dam': 1636, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 17:59:53,888 (5911103) INFO root [<module>:35] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {'not_a_dam': 7169, 'dam': 0}\n",
      "validation_log {'not_a_dam': 1831, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:00:28,162 (5945376) INFO root [<module>:35] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {'not_a_dam': 7984, 'dam': 0}\n",
      "validation_log {'not_a_dam': 2016, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:01:02,273 (5979487) INFO root [<module>:35] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {'not_a_dam': 8788, 'dam': 0}\n",
      "validation_log {'not_a_dam': 2212, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:01:35,368 (6012583) INFO root [<module>:35] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n",
      "training_log {'not_a_dam': 9583, 'dam': 0}\n",
      "validation_log {'not_a_dam': 2417, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:02:08,977 (6046192) INFO root [<module>:35] tf_record_iteration 12\n",
      "Starting to make TFRecords 12\n",
      "training_log {'not_a_dam': 10382, 'dam': 0}\n",
      "validation_log {'not_a_dam': 2618, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:02:42,405 (6079620) INFO root [<module>:35] tf_record_iteration 13\n",
      "Starting to make TFRecords 13\n",
      "training_log {'not_a_dam': 11188, 'dam': 0}\n",
      "validation_log {'not_a_dam': 2812, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:03:14,259 (6111473) INFO root [<module>:35] tf_record_iteration 14\n",
      "Starting to make TFRecords 14\n",
      "training_log {'not_a_dam': 11983, 'dam': 0}\n",
      "validation_log {'not_a_dam': 3017, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:03:46,419 (6143634) INFO root [<module>:35] tf_record_iteration 15\n",
      "Starting to make TFRecords 15\n",
      "training_log {'not_a_dam': 12765, 'dam': 0}\n",
      "validation_log {'not_a_dam': 3235, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:04:19,442 (6176656) INFO root [<module>:35] tf_record_iteration 16\n",
      "Starting to make TFRecords 16\n",
      "training_log {'not_a_dam': 13573, 'dam': 0}\n",
      "validation_log {'not_a_dam': 3427, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:04:52,359 (6209573) INFO root [<module>:35] tf_record_iteration 17\n",
      "Starting to make TFRecords 17\n",
      "training_log {'not_a_dam': 14374, 'dam': 0}\n",
      "validation_log {'not_a_dam': 3626, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:05:25,831 (6243046) INFO root [<module>:35] tf_record_iteration 18\n",
      "Starting to make TFRecords 18\n",
      "training_log {'not_a_dam': 15168, 'dam': 0}\n",
      "validation_log {'not_a_dam': 3832, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:05:59,414 (6276629) INFO root [<module>:35] tf_record_iteration 19\n",
      "Starting to make TFRecords 19\n",
      "training_log {'not_a_dam': 15994, 'dam': 0}\n",
      "validation_log {'not_a_dam': 4006, 'dam': 0}\n",
      "\n",
      "\n",
      "2019-08-26 18:06:32,998 (6310213) INFO root [<module>:35] tf_record_iteration 20\n",
      "Starting to make TFRecords 20\n",
      "training_log {'not_a_dam': 16722, 'dam': 0}\n",
      "validation_log {'not_a_dam': 4193, 'dam': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_images_file_list = not_a_dam_file_list\n",
    "\n",
    "\n",
    "\n",
    "# Make workspace outputs directories doesn't exist\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set')]\n",
    "\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Do the thing    \n",
    "training_log = {'dam': 0, 'not_a_dam': 0}\n",
    "validation_log = {'dam': 0, 'not_a_dam': 0}\n",
    "\n",
    "\n",
    "#last_time = time.time()\n",
    "\n",
    "training_writer_count = 0\n",
    "validation_writer_count = 0\n",
    "tf_record_iteration = 0\n",
    "max_tf_record_iteration = int(len(all_images_file_list)/Dams_per_round)\n",
    "\n",
    "\n",
    "\n",
    "while tf_record_iteration <= max_tf_record_iteration:\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        LOGGER.info('tf_record_iteration %d' % tf_record_iteration)\n",
    "\n",
    "        # Open writers\n",
    "        training_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'training_set/not_a_dam_%d.record' % tf_record_iteration))\n",
    "        validation_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'validation_set/not_a_dam_%d.record' % tf_record_iteration))\n",
    "        \n",
    "        # Get a slice of the dataset\n",
    "        slice_dam_images_list = all_images_file_list[tf_record_iteration*Dams_per_round:min((tf_record_iteration+1)*Dams_per_round,len(all_images_file_list))]\n",
    "\n",
    "        #This is where I make TFRecords!\n",
    "        training_log, validation_log = make_TFRecords(slice_dam_images_list,tf_record_iteration)\n",
    "\n",
    "        # Close writers\n",
    "        training_writer.close()\n",
    "        validation_writer.close()\n",
    "\n",
    "        # Advance loop\n",
    "        tf_record_iteration += 1\n",
    "\n",
    "        print('training_log',training_log)\n",
    "        print('validation_log',validation_log)\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "#@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check files created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../../data/TFRecords_not_a_dam_08-26/validation_set/*'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(os.path.join(path_to_charlie_root,'data/TFRecords_not_a_dam_08-26',split_set)+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 21 files...\n",
      "...Done\n",
      "Checking 21 files...\n",
      "...Done\n"
     ]
    }
   ],
   "source": [
    "# Check files created\n",
    "\n",
    "for split_set in ['training_set','validation_set']:\n",
    "    files_to_check = glob.glob(os.path.join(path_to_charlie_root,'data/TFRecords_not_a_dam_08-26',split_set)+'/*')\n",
    "    print('Checking '+str(len(files_to_check))+' files...')\n",
    "\n",
    "    for filename in files_to_check:\n",
    "\n",
    "        try:\n",
    "            for example in tf.python_io.tf_record_iterator(filename): \n",
    "                tf_example = tf.train.Example.FromString(example) \n",
    "        except:\n",
    "            print(filename, ' is truncated')\n",
    "    print('...Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative to shapefiles of South Africa - harcoding lat/lon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords_withSouthAf_rectangle(dam_file_list,tf_record_iteration):\n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "\n",
    "    for image_path in dam_file_list:\n",
    "#             current_time = time.time()\n",
    "#             if current_time - last_time > 5.0:\n",
    "#                 LOGGER.info('training_log: %s', training_log)\n",
    "#                 LOGGER.info('validation_log: %s', validation_log)\n",
    "#                 LOGGER.info('southaf_log: %s', southaf_log)\n",
    "#                 LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "#                 LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "#                 last_time = current_time\n",
    "\n",
    "        #  Note from Rich:\n",
    "        # looks like anything can be used here, including serializing\n",
    "        # a tensor tf.serialize_tensor\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "\n",
    "        # if this image is a dam:\n",
    "\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if os.path.exists(json_path):\n",
    "            dam_type = b'dam'\n",
    "\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax >= 1 or ymax >= 1):\n",
    "                LOGGER.warning('bounding box out of bounds %s %s %s %s',\n",
    "                               xmin, xmax, ymin, ymax)\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = b'not_a_dam'\n",
    "            \n",
    "            json_path = image_path.replace('_bb.png', '.json')\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            \n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "\n",
    "        \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "\n",
    "        # For now only have centroids for dams (no lat/lon for not_a_dams, so fixinf to 99999 NoData for now) !!\n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "        \n",
    "        # WHEN GDAL WORKS:\n",
    "        #centroid = image_metadata['lng_lat_centroid']\n",
    "        #if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # removed: and dam_type == b'dam'\n",
    "        # Till then, hard code:\n",
    "\n",
    "\n",
    "        #South Africa coordinates\n",
    "        southaf_max_lon = -26.958585\n",
    "        southaf_max_lat = 29.682244\n",
    "        southaf_min_lon = -28.756566\n",
    "        southaf_min_lat = 19.959007\n",
    "\n",
    "        if ((southaf_min_lon < centroid[0] < southaf_max_lon) and\n",
    "            (southaf_min_lat < centroid[1] < southaf_max_lat)):\n",
    "            writer = southaf_writer\n",
    "            log = southaf_log\n",
    "        elif numpy.random.random() > dev_set_portion:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        #elif POSSIBLY ADD MORE TEST SET HERE?\n",
    "        #    writer = test_writer\n",
    "        #    log = test_log\n",
    "        else:\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich's orginial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Entry point.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(WORKSPACE_DIR)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    training_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "    validation_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "    south_africa_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "\n",
    "    last_time = time.time()\n",
    "    training_writer_count = 0\n",
    "    validation_writer_count = 0\n",
    "    tf_record_iteration = 0\n",
    "\n",
    "    ## Subsets for Dev\n",
    "    dam_file_iter = glob.iglob(os.path.join(DAM_IMAGE_DIR, '119*pped.png'))\n",
    "    no_a_dam_file_iter = glob.iglob(os.path.join(NOT_A_DAM_IMAGE_DIR, '350*.png'))\n",
    "    \n",
    "    ## REAL DATASET FULL\n",
    "    # dam_file_iter = glob.iglob(os.path.join(DAM_IMAGE_DIR, '*pped.png'))\n",
    "    # no_a_dam_file_iter = glob.iglob(os.path.join(NOT_A_DAM_IMAGE_DIR, '*.png'))\n",
    "    \n",
    "    #  NOT USING THIS BELOW BECAUSE NOT DOING 50/50.\n",
    "    #dam_list_iter = [\n",
    "    #    path for path_tuple in zip(dam_file_iter, no_a_dam_file_iter)\n",
    "    #    for path in path_tuple]\n",
    "\n",
    "    #while True:\n",
    "    #    # this makes DAMS_PER_RECORD list of files\n",
    "    #    dam_file_list = [\n",
    "    #        (path, dam_type) for path, dam_type in zip(\n",
    "    #            itertools.islice(\n",
    "    #                dam_list_iter,\n",
    "    #                DAMS_PER_RECORD*tf_record_iteration,\n",
    "    #                DAMS_PER_RECORD*(tf_record_iteration+1)),\n",
    "    #            itertools.cycle([b'dam', b'not_a_dam']))]\n",
    "\n",
    "        LOGGER.debug(dam_file_list)\n",
    "        if not dam_file_list:\n",
    "            break\n",
    "\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "            training_writer = tf.python_io.TFRecordWriter(\n",
    "                os.path.join(\n",
    "                    WORKSPACE_DIR,\n",
    "                    'dam_training_%d.record' % tf_record_iteration))\n",
    "            validation_writer = tf.python_io.TFRecordWriter(\n",
    "                os.path.join(\n",
    "                    WORKSPACE_DIR,\n",
    "                    'dam_validation_%d.record' % tf_record_iteration))\n",
    "            south_africa_writer = tf.python_io.TFRecordWriter(os.path.join(\n",
    "                WORKSPACE_DIR, 'south_africa_%d.record' %\n",
    "                tf_record_iteration))\n",
    "\n",
    "            for image_path, dam_type in dam_file_list:\n",
    "                current_time = time.time()\n",
    "                if current_time - last_time > 5.0:\n",
    "                    LOGGER.info('training_log: %s', training_log)\n",
    "                    LOGGER.info('validation_log: %s', validation_log)\n",
    "                    LOGGER.info('south_africa_log: %s', south_africa_log)\n",
    "                    LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "                    LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "                    last_time = current_time\n",
    "                # looks like anything can be used here, including serializing\n",
    "                # a tensor tf.serialize_tensor\n",
    "                image_string = tf.read_file(image_path)\n",
    "                image_decoded = tf.image.decode_png(image_string).eval()\n",
    "                image_string = open(image_path, 'rb').read()\n",
    "                feature_dict = {\n",
    "                    'image/height': int64_feature(\n",
    "                        image_decoded.shape[0]),\n",
    "                    'image/width': int64_feature(\n",
    "                        image_decoded.shape[1]),\n",
    "                    'image/filename': bytes_feature(\n",
    "                        bytes(image_path, 'utf8')),\n",
    "                    'image/source_id': bytes_feature(\n",
    "                        bytes(image_path, 'utf8')),\n",
    "                    'image/encoded': bytes_feature(image_string),\n",
    "                    'image/format': bytes_feature(b'png'),\n",
    "                }\n",
    "                if dam_type == b'dam':\n",
    "                    json_path = image_path.replace('.png', '.json')\n",
    "                    with open(json_path, 'r') as json_file:\n",
    "                        image_metadata = json.load(json_file)\n",
    "                    xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "                    xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "                    ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "                    ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "                    if (xmin < 0 or ymin < 0 or\n",
    "                            xmax >= 1 or\n",
    "                            ymax >= 1):\n",
    "                        LOGGER.warning(\n",
    "                            'bounding box out of bounds %s %s %s %s',\n",
    "                            xmin, xmax, ymin, ymax)\n",
    "                        xmin = max(0, xmin)\n",
    "                        xmax = min(xmax, 1)\n",
    "                        ymin = max(0, ymin)\n",
    "                        ymax = min(ymax, 1)\n",
    "\n",
    "                    feature_dict.update({\n",
    "                        'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                        'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                        'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                        'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                        'image/object/class/label': int64_list_feature(\n",
    "                            [1]),  # the '1' is type 1 which is a dam\n",
    "                        'image/object/class/text': bytes_list_feature(\n",
    "                            [b'dam']),\n",
    "                    })\n",
    "                    tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                        feature=feature_dict))\n",
    "\n",
    "                    centroid = image_metadata['lng_lat_centroid']\n",
    "                    if dam_type == b'dam' and sa_geom_prep.contains(\n",
    "                            shapely.geometry.Point(centroid[0], centroid[1])):\n",
    "                        writer = south_africa_writer\n",
    "                        log = south_africa_log\n",
    "                        writer.write(tf_record.SerializeToString())\n",
    "                        log[dam_type] += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                        feature=feature_dict))\n",
    "                if numpy.random.random() > dev_set_portion:\n",
    "                    writer = training_writer\n",
    "                    log = training_log\n",
    "                else:\n",
    "                    writer = validation_writer\n",
    "                    log = validation_log\n",
    "                writer.write(tf_record.SerializeToString())\n",
    "                log[dam_type] += 1\n",
    "\n",
    "            LOGGER.info(\n",
    "                \"training writer full creating %d instance\" %\n",
    "                tf_record_iteration)\n",
    "            tf_record_iteration += 1\n",
    "            training_writer.close()\n",
    "            validation_writer.close()\n",
    "            south_africa_writer.close()\n",
    "\n",
    "    with open('write_stats.txt', 'w') as write_stats_file:\n",
    "        write_stats_file.write(\n",
    "            f\"\"\"validation: dam({validation_log[b'dam']}) not_a_dam({\n",
    "                validation_log[b'not_a_dam']})\\n\"\"\"\n",
    "            f\"\"\"training: dam({training_log[b'dam']}) not_a_dam({\n",
    "                training_log[b'not_a_dam']})\\n\"\"\"\n",
    "            f\"\"\"south_africa: dam({south_africa_log[b'dam']}) not_a_dam({\n",
    "                south_africa_log[b'not_a_dam']})\\n\"\"\")\n",
    "\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
