{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import itertools\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import random\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(900)\n",
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "LOGGER.info(\"Logger in INFO mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "REQUEST_TIMEOUT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=('%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n",
    "                            ' [%(funcName)s:%(lineno)d] %(message)s'),\n",
    "                    stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_charlie_root = \"../../../..\"\n",
    "NOT_A_DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019_cropped/not_a_dam_images\")\n",
    "DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019_cropped/dam_images\")\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "if not os.path.exists(DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % DAM_IMAGE_DIR)\n",
    "OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/TFRecords_imagery_6-7_cropped_made_7-22\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('not_a_dam.png')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30337"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5899"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36236"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_file_list = dam_file_list+not_a_dam_file_list\n",
    "random.seed(0)\n",
    "random.shuffle(all_images_file_list)\n",
    "len(all_images_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set_portion = .2\n",
    "validation_set_portion = .15\n",
    "Dams_per_round = 1000 # = max_dams_per_record \n",
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "southaf_json_paths = os.listdir(os.path.join(path_to_charlie_root, 'data/imagery-6-7-2019_Cropped/southaf_set/json'))\n",
    "if '122774_clipped.json' in southaf_json_paths:\n",
    "    print('true')\n",
    "else:\n",
    "    print('false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords(dam_file_list,tf_record_iteration):    \n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "    random.seed(tf_record_iteration)\n",
    "    random_numbers_list = [random.random() for x in range(0, len(dam_file_list))]\n",
    "    random_number_iterator = 0\n",
    "    for image_path in dam_file_list:\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "        # if this image is a dam:\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if not os.path.exists(json_path):\n",
    "            raise ValueError(\"can't find %s'\" % json_path)\n",
    "        if not 'not_a_dam' in image_path:\n",
    "            dam_type = 'dam'\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax > 1 or ymax > 1):\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = 'not_a_dam'\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        random_number = random_numbers_list[random_number_iterator]\n",
    "        random_number_iterator+=1\n",
    "        json_path_basename = os.path.basename(json_path)\n",
    "        if json_path_basename in southaf_json_paths: # both for dams & not_a_dams\n",
    "            writer = southaf_writer\n",
    "            log = southaf_log\n",
    "        elif random_number < holdout_set_portion:\n",
    "            writer = test_writer\n",
    "            log = test_log\n",
    "        elif random_number > (1-validation_set_portion):\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-22 22:17:10,789 (144875) INFO root [<module>:32] tf_record_iteration 0\n",
      "Starting to make TFRecords 0\n",
      "training_log {'dam': 540, 'not_a_dam': 107}\n",
      "validation_log {'dam': 116, 'not_a_dam': 25}\n",
      "test_log {'dam': 164, 'not_a_dam': 35}\n",
      "southaf_log {'dam': 11, 'not_a_dam': 2}\n",
      "\n",
      "\n",
      "2019-07-22 22:19:27,245 (281331) INFO root [<module>:32] tf_record_iteration 1\n",
      "Starting to make TFRecords 1\n",
      "training_log {'dam': 1071, 'not_a_dam': 224}\n",
      "validation_log {'dam': 246, 'not_a_dam': 56}\n",
      "test_log {'dam': 317, 'not_a_dam': 61}\n",
      "southaf_log {'dam': 21, 'not_a_dam': 4}\n",
      "\n",
      "\n",
      "2019-07-22 22:21:43,338 (417424) INFO root [<module>:32] tf_record_iteration 2\n",
      "Starting to make TFRecords 2\n",
      "training_log {'dam': 1614, 'not_a_dam': 323}\n",
      "validation_log {'dam': 372, 'not_a_dam': 73}\n",
      "test_log {'dam': 487, 'not_a_dam': 82}\n",
      "southaf_log {'dam': 37, 'not_a_dam': 12}\n",
      "\n",
      "\n",
      "2019-07-22 22:23:54,323 (548409) INFO root [<module>:32] tf_record_iteration 3\n",
      "Starting to make TFRecords 3\n",
      "training_log {'dam': 2148, 'not_a_dam': 418}\n",
      "validation_log {'dam': 497, 'not_a_dam': 96}\n",
      "test_log {'dam': 656, 'not_a_dam': 120}\n",
      "southaf_log {'dam': 50, 'not_a_dam': 15}\n",
      "\n",
      "\n",
      "2019-07-22 22:26:03,555 (677641) INFO root [<module>:32] tf_record_iteration 4\n",
      "Starting to make TFRecords 4\n",
      "training_log {'dam': 2657, 'not_a_dam': 512}\n",
      "validation_log {'dam': 643, 'not_a_dam': 123}\n",
      "test_log {'dam': 830, 'not_a_dam': 153}\n",
      "southaf_log {'dam': 63, 'not_a_dam': 19}\n",
      "\n",
      "\n",
      "2019-07-22 22:28:09,514 (803600) INFO root [<module>:32] tf_record_iteration 5\n",
      "Starting to make TFRecords 5\n",
      "training_log {'dam': 3194, 'not_a_dam': 612}\n",
      "validation_log {'dam': 757, 'not_a_dam': 151}\n",
      "test_log {'dam': 1005, 'not_a_dam': 188}\n",
      "southaf_log {'dam': 72, 'not_a_dam': 21}\n",
      "\n",
      "\n",
      "2019-07-22 22:30:16,175 (930261) INFO root [<module>:32] tf_record_iteration 6\n",
      "Starting to make TFRecords 6\n",
      "training_log {'dam': 3715, 'not_a_dam': 706}\n",
      "validation_log {'dam': 880, 'not_a_dam': 176}\n",
      "test_log {'dam': 1203, 'not_a_dam': 214}\n",
      "southaf_log {'dam': 81, 'not_a_dam': 25}\n",
      "\n",
      "\n",
      "2019-07-22 22:32:31,525 (1065611) INFO root [<module>:32] tf_record_iteration 7\n",
      "Starting to make TFRecords 7\n",
      "training_log {'dam': 4236, 'not_a_dam': 812}\n",
      "validation_log {'dam': 1003, 'not_a_dam': 200}\n",
      "test_log {'dam': 1376, 'not_a_dam': 253}\n",
      "southaf_log {'dam': 91, 'not_a_dam': 29}\n",
      "\n",
      "\n",
      "2019-07-22 22:34:44,637 (1198723) INFO root [<module>:32] tf_record_iteration 8\n",
      "Starting to make TFRecords 8\n",
      "training_log {'dam': 4785, 'not_a_dam': 909}\n",
      "validation_log {'dam': 1119, 'not_a_dam': 232}\n",
      "test_log {'dam': 1528, 'not_a_dam': 286}\n",
      "southaf_log {'dam': 109, 'not_a_dam': 32}\n",
      "\n",
      "\n",
      "2019-07-22 22:36:56,191 (1330277) INFO root [<module>:32] tf_record_iteration 9\n",
      "Starting to make TFRecords 9\n",
      "training_log {'dam': 5279, 'not_a_dam': 1018}\n",
      "validation_log {'dam': 1238, 'not_a_dam': 252}\n",
      "test_log {'dam': 1718, 'not_a_dam': 327}\n",
      "southaf_log {'dam': 132, 'not_a_dam': 36}\n",
      "\n",
      "\n",
      "2019-07-22 22:39:03,758 (1457844) INFO root [<module>:32] tf_record_iteration 10\n",
      "Starting to make TFRecords 10\n",
      "training_log {'dam': 5829, 'not_a_dam': 1117}\n",
      "validation_log {'dam': 1347, 'not_a_dam': 283}\n",
      "test_log {'dam': 1886, 'not_a_dam': 358}\n",
      "southaf_log {'dam': 143, 'not_a_dam': 37}\n",
      "\n",
      "\n",
      "2019-07-22 22:41:13,212 (1587298) INFO root [<module>:32] tf_record_iteration 11\n",
      "Starting to make TFRecords 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cb48f7fef2a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m#This is where I make TFRecords!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mtraining_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msouthaf_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_TFRecords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_dam_images_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf_record_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Close writers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-dab594305aa3>\u001b[0m in \u001b[0;36mmake_TFRecords\u001b[0;34m(dam_file_list, tf_record_iteration)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdam_file_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimage_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimage_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         feature_dict = {\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   5179\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5180\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 5181\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make workspace outputs directories doesn't exist\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'southaf_set')]\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "# Do the thing    \n",
    "training_log = {'dam': 0, 'not_a_dam': 0}\n",
    "validation_log = {'dam': 0, 'not_a_dam': 0}\n",
    "test_log = {'dam': 0, 'not_a_dam': 0}\n",
    "southaf_log = {'dam': 0, 'not_a_dam': 0}\n",
    "\n",
    "#last_time = time.time()\n",
    "\n",
    "training_writer_count = 0\n",
    "validation_writer_count = 0\n",
    "tf_record_iteration = 0\n",
    "max_tf_record_iteration = int(len(all_images_file_list)/Dams_per_round)\n",
    "\n",
    "\n",
    "\n",
    "while tf_record_iteration <= max_tf_record_iteration:\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "        LOGGER.info('tf_record_iteration %d' % tf_record_iteration)\n",
    "\n",
    "        # Open writers\n",
    "        training_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'training_set/dams_%d.record' % tf_record_iteration))\n",
    "        validation_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'validation_set/dams_%d.record' % tf_record_iteration))\n",
    "        test_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'test_set/dams_%d.record' % tf_record_iteration))\n",
    "        southaf_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'southaf_set/dams_%d.record' % tf_record_iteration))\n",
    "\n",
    "        # Get a slice of the dataset\n",
    "        slice_dam_images_list = all_images_file_list[tf_record_iteration*Dams_per_round:min((tf_record_iteration+1)*Dams_per_round,len(all_images_file_list))]\n",
    "\n",
    "        #This is where I make TFRecords!\n",
    "        training_log, validation_log, test_log, southaf_log = make_TFRecords(slice_dam_images_list,tf_record_iteration)\n",
    "\n",
    "        # Close writers\n",
    "        training_writer.close()\n",
    "        validation_writer.close()\n",
    "        southaf_writer.close()\n",
    "        test_writer.close()\n",
    "\n",
    "        # Advance loop\n",
    "        tf_record_iteration += 1\n",
    "\n",
    "        print('training_log',training_log)\n",
    "        print('validation_log',validation_log)\n",
    "        print('test_log',test_log)\n",
    "        print('southaf_log',southaf_log)\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "#@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
