{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning sunshine\n"
     ]
    }
   ],
   "source": [
    "print('Good morning sunshine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script requires TF + gdal + pip install request, shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# Installed in addition\n",
    "import requests\n",
    "\n",
    "## Useless?\n",
    "from osgeo import gdal\n",
    "import ogr\n",
    "import shapely.wkb\n",
    "import shapely.prepared\n",
    "#from retrying import retry\n",
    "\n",
    "import random\n",
    "random.seed(900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.3\n"
     ]
    }
   ],
   "source": [
    "import osgeo\n",
    "print(osgeo.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger()\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "LOGGER.info(\"Logger in INFO mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "LOGGER.debug(\"Logger in DEBUG mode\")\n",
    "\n",
    "REQUEST_TIMEOUT = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.ERROR,\n",
    "                    format=('%(asctime)s (%(relativeCreated)d) %(levelname)s %(name)s'\n",
    "                            ' [%(funcName)s:%(lineno)d] %(message)s'),\n",
    "                    stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_charlie_root = \"../../..\"\n",
    "NOT_A_DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/not_a_dam_images\")\n",
    "DAM_IMAGE_DIR = os.path.join(path_to_charlie_root,\"data/imagery-6-7-2019/dam_images\")\n",
    "\n",
    "TM_WORLD_BORDERS_URL = 'https://storage.googleapis.com/ecoshard-root/ipbes/TM_WORLD_BORDERS_SIMPL-0.3_md5_15057f7b17752048f9bd2e2e607fe99c.zip'\n",
    "\n",
    "if not os.path.exists(NOT_A_DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % NOT_A_DAM_IMAGE_DIR)\n",
    "if not os.path.exists(DAM_IMAGE_DIR):\n",
    "    raise ValueError(\"can't find %s'\" % DAM_IMAGE_DIR)\n",
    "    \n",
    "OUTPUTS_DIR = os.path.join(path_to_charlie_root,\"data/TFRecords_imagery6-7_made6-14\")\n",
    "WORKSPACE_DIR = OUTPUTS_DIR\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run just one of the 2 cells below! (full dataset or subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsets of data inputs - for faster development purposes\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('5140_clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('362_not_a_dam_bb.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full dataset\n",
    "\n",
    "dam_file_list = [os.path.join(DAM_IMAGE_DIR, f)\n",
    "                 for f in os.listdir(DAM_IMAGE_DIR) if f.endswith('clipped.png')]\n",
    "not_a_dam_file_list = [os.path.join(NOT_A_DAM_IMAGE_DIR, f)\n",
    "                       for f in os.listdir(NOT_A_DAM_IMAGE_DIR) if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30337"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dam_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11798"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_a_dam_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do run this one to merge dam_list and not_a_dam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images_file_list = dam_file_list+not_a_dam_file_list\n",
    "random.shuffle(all_images_file_list)\n",
    "len(all_images_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_set_portion = .2\n",
    "validation_set_portion = .15\n",
    "DAMS_PER_RECORD = 200 # 5 in dev mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def int64_list_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "def bytes_list_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "def float_list_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get South Africa geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url_to_file(url, target_file_path):\n",
    "    \"\"\"Use requests to download a file.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): url to file.\n",
    "        target_file_path (string): local path to download the file.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=REQUEST_TIMEOUT)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(target_file_path))\n",
    "        except OSError:\n",
    "            pass\n",
    "        with open(target_file_path, 'wb') as target_file:\n",
    "            shutil.copyfileobj(response.raw, target_file)\n",
    "        del response\n",
    "    except:\n",
    "        LOGGER.exception('download of {url} to {target_file_path} failed')\n",
    "        # mods from LOGGER.exception(f'download of {url} to {target_file_path} failed')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_world_borders_zip_path = os.path.join(\n",
    "        WORKSPACE_DIR, os.path.basename(TM_WORLD_BORDERS_URL))\n",
    "if not os.path.exists(tm_world_borders_zip_path):\n",
    "    download_url_to_file(TM_WORLD_BORDERS_URL, tm_world_borders_zip_path)\n",
    "    with zipfile.ZipFile(tm_world_borders_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(WORKSPACE_DIR)\n",
    "\n",
    "tm_world_borders_vector_path = os.path.join(\n",
    "    WORKSPACE_DIR, 'TM_WORLD_BORDERS-0.3.shp')\n",
    "#tm_world_borders_vector = gdal.Open(ogr.Open(tm_world_borders_vector_path)) # Changed OpenEx to Open.\n",
    "    #tm_world_borders_vector_path,ogr.Open(path))#, gdal.OF_VECTOR)\n",
    "tm_world_borders_vector = ogr.Open(tm_world_borders_vector_path)\n",
    "tm_world_borders_layer = tm_world_borders_vector.GetLayer()\n",
    "for border_feature in tm_world_borders_layer:\n",
    "    if border_feature.GetField('NAME') == 'South Africa':\n",
    "        sa_geom = border_feature.GetGeometryRef()\n",
    "        sa_geom_prep = shapely.prepared.prep(\n",
    "            shapely.wkb.loads(sa_geom.ExportToWkb()))\n",
    "        break\n",
    "LOGGER.debug(sa_geom_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative for gdal issues: hard coding South Af \"recatangle\" (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Make TFRecords !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: Function to make_TFRecords from a list of images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords(dam_file_list,tf_record_iteration):\n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "\n",
    "    for image_path in dam_file_list:\n",
    "#             current_time = time.time()\n",
    "#             if current_time - last_time > 5.0:\n",
    "#                 LOGGER.info('training_log: %s', training_log)\n",
    "#                 LOGGER.info('validation_log: %s', validation_log)\n",
    "#                 LOGGER.info('southaf_log: %s', southaf_log)\n",
    "#                 LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "#                 LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "#                 last_time = current_time\n",
    "\n",
    "        #  Note from Rich:\n",
    "        # looks like anything can be used here, including serializing\n",
    "        # a tensor tf.serialize_tensor\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "\n",
    "        # if this image is a dam:\n",
    "\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if os.path.exists(json_path):\n",
    "            dam_type = b'dam'\n",
    "\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax >= 1 or ymax >= 1):\n",
    "                LOGGER.warning('bounding box out of bounds %s %s %s %s',\n",
    "                               xmin, xmax, ymin, ymax)\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = b'not_a_dam'\n",
    "            \n",
    "            json_path = image_path.replace('_bb.png', '.json')\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            \n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "\n",
    "        \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "\n",
    "        # For now only have centroids for dams (no lat/lon for not_a_dams, so fixinf to 99999 NoData for now) !!\n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "            \n",
    "        random_number = numpy.random.random()\n",
    "        if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # both for dams & not_a_dams\n",
    "            writer = southaf_writer\n",
    "            log = southaf_log\n",
    "        elif random_number < holdout_set_portion:\n",
    "            writer = test_writer\n",
    "            log = test_log\n",
    "        elif random_number > (1-validation_set_portion):\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        else:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to make TFRecords 0\n",
      "training_log {b'not_a_dam': 13, b'dam': 108}\n",
      "validation_log {b'not_a_dam': 4, b'dam': 34}\n",
      "test_log {b'not_a_dam': 5, b'dam': 33}\n",
      "southaf_log {b'not_a_dam': 1, b'dam': 2}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 1\n",
      "training_log {b'not_a_dam': 32, b'dam': 207}\n",
      "validation_log {b'not_a_dam': 11, b'dam': 61}\n",
      "test_log {b'not_a_dam': 11, b'dam': 70}\n",
      "southaf_log {b'not_a_dam': 2, b'dam': 6}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 2\n",
      "training_log {b'not_a_dam': 50, b'dam': 306}\n",
      "validation_log {b'not_a_dam': 14, b'dam': 89}\n",
      "test_log {b'not_a_dam': 15, b'dam': 111}\n",
      "southaf_log {b'not_a_dam': 3, b'dam': 12}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 3\n",
      "training_log {b'not_a_dam': 69, b'dam': 401}\n",
      "validation_log {b'not_a_dam': 21, b'dam': 115}\n",
      "test_log {b'not_a_dam': 24, b'dam': 151}\n",
      "southaf_log {b'not_a_dam': 4, b'dam': 15}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 4\n",
      "training_log {b'not_a_dam': 89, b'dam': 520}\n",
      "validation_log {b'not_a_dam': 27, b'dam': 141}\n",
      "test_log {b'not_a_dam': 29, b'dam': 169}\n",
      "southaf_log {b'not_a_dam': 6, b'dam': 19}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 5\n",
      "training_log {b'not_a_dam': 102, b'dam': 633}\n",
      "validation_log {b'not_a_dam': 29, b'dam': 166}\n",
      "test_log {b'not_a_dam': 34, b'dam': 209}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 20}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 6\n",
      "training_log {b'not_a_dam': 124, b'dam': 738}\n",
      "validation_log {b'not_a_dam': 31, b'dam': 194}\n",
      "test_log {b'not_a_dam': 36, b'dam': 246}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 24}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 7\n",
      "training_log {b'not_a_dam': 140, b'dam': 851}\n",
      "validation_log {b'not_a_dam': 35, b'dam': 222}\n",
      "test_log {b'not_a_dam': 43, b'dam': 274}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 28}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 8\n",
      "training_log {b'not_a_dam': 161, b'dam': 942}\n",
      "validation_log {b'not_a_dam': 40, b'dam': 256}\n",
      "test_log {b'not_a_dam': 51, b'dam': 310}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 33}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 9\n",
      "training_log {b'not_a_dam': 178, b'dam': 1051}\n",
      "validation_log {b'not_a_dam': 43, b'dam': 283}\n",
      "test_log {b'not_a_dam': 55, b'dam': 350}\n",
      "southaf_log {b'not_a_dam': 7, b'dam': 33}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 10\n",
      "training_log {b'not_a_dam': 193, b'dam': 1156}\n",
      "validation_log {b'not_a_dam': 50, b'dam': 306}\n",
      "test_log {b'not_a_dam': 61, b'dam': 384}\n",
      "southaf_log {b'not_a_dam': 9, b'dam': 41}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 11\n",
      "training_log {b'not_a_dam': 214, b'dam': 1261}\n",
      "validation_log {b'not_a_dam': 55, b'dam': 333}\n",
      "test_log {b'not_a_dam': 67, b'dam': 412}\n",
      "southaf_log {b'not_a_dam': 11, b'dam': 47}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 12\n",
      "training_log {b'not_a_dam': 232, b'dam': 1375}\n",
      "validation_log {b'not_a_dam': 61, b'dam': 361}\n",
      "test_log {b'not_a_dam': 71, b'dam': 439}\n",
      "southaf_log {b'not_a_dam': 11, b'dam': 50}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 13\n",
      "training_log {b'not_a_dam': 246, b'dam': 1483}\n",
      "validation_log {b'not_a_dam': 66, b'dam': 391}\n",
      "test_log {b'not_a_dam': 77, b'dam': 474}\n",
      "southaf_log {b'not_a_dam': 11, b'dam': 52}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 14\n",
      "training_log {b'not_a_dam': 276, b'dam': 1586}\n",
      "validation_log {b'not_a_dam': 72, b'dam': 421}\n",
      "test_log {b'not_a_dam': 81, b'dam': 496}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 56}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 15\n",
      "training_log {b'not_a_dam': 292, b'dam': 1705}\n",
      "validation_log {b'not_a_dam': 78, b'dam': 444}\n",
      "test_log {b'not_a_dam': 85, b'dam': 525}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 59}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 16\n",
      "training_log {b'not_a_dam': 314, b'dam': 1809}\n",
      "validation_log {b'not_a_dam': 80, b'dam': 469}\n",
      "test_log {b'not_a_dam': 94, b'dam': 562}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 60}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 17\n",
      "training_log {b'not_a_dam': 330, b'dam': 1925}\n",
      "validation_log {b'not_a_dam': 82, b'dam': 496}\n",
      "test_log {b'not_a_dam': 99, b'dam': 593}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 63}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 18\n",
      "training_log {b'not_a_dam': 356, b'dam': 2034}\n",
      "validation_log {b'not_a_dam': 92, b'dam': 512}\n",
      "test_log {b'not_a_dam': 106, b'dam': 624}\n",
      "southaf_log {b'not_a_dam': 12, b'dam': 64}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 19\n",
      "training_log {b'not_a_dam': 373, b'dam': 2139}\n",
      "validation_log {b'not_a_dam': 97, b'dam': 543}\n",
      "test_log {b'not_a_dam': 110, b'dam': 657}\n",
      "southaf_log {b'not_a_dam': 13, b'dam': 68}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 20\n",
      "training_log {b'not_a_dam': 390, b'dam': 2254}\n",
      "validation_log {b'not_a_dam': 102, b'dam': 573}\n",
      "test_log {b'not_a_dam': 116, b'dam': 682}\n",
      "southaf_log {b'not_a_dam': 13, b'dam': 70}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 21\n",
      "training_log {b'not_a_dam': 407, b'dam': 2373}\n",
      "validation_log {b'not_a_dam': 107, b'dam': 594}\n",
      "test_log {b'not_a_dam': 121, b'dam': 714}\n",
      "southaf_log {b'not_a_dam': 13, b'dam': 71}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 22\n",
      "training_log {b'not_a_dam': 422, b'dam': 2486}\n",
      "validation_log {b'not_a_dam': 110, b'dam': 614}\n",
      "test_log {b'not_a_dam': 126, b'dam': 755}\n",
      "southaf_log {b'not_a_dam': 13, b'dam': 74}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 23\n",
      "training_log {b'not_a_dam': 442, b'dam': 2606}\n",
      "validation_log {b'not_a_dam': 114, b'dam': 634}\n",
      "test_log {b'not_a_dam': 130, b'dam': 785}\n",
      "southaf_log {b'not_a_dam': 14, b'dam': 75}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 24\n",
      "training_log {b'not_a_dam': 461, b'dam': 2716}\n",
      "validation_log {b'not_a_dam': 118, b'dam': 659}\n",
      "test_log {b'not_a_dam': 133, b'dam': 822}\n",
      "southaf_log {b'not_a_dam': 15, b'dam': 76}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 25\n",
      "training_log {b'not_a_dam': 470, b'dam': 2830}\n",
      "validation_log {b'not_a_dam': 122, b'dam': 685}\n",
      "test_log {b'not_a_dam': 142, b'dam': 855}\n",
      "southaf_log {b'not_a_dam': 16, b'dam': 80}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 26\n",
      "training_log {b'not_a_dam': 489, b'dam': 2938}\n",
      "validation_log {b'not_a_dam': 128, b'dam': 712}\n",
      "test_log {b'not_a_dam': 147, b'dam': 889}\n",
      "southaf_log {b'not_a_dam': 16, b'dam': 81}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 27\n",
      "training_log {b'not_a_dam': 504, b'dam': 3048}\n",
      "validation_log {b'not_a_dam': 135, b'dam': 740}\n",
      "test_log {b'not_a_dam': 152, b'dam': 922}\n",
      "southaf_log {b'not_a_dam': 16, b'dam': 83}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 28\n",
      "training_log {b'not_a_dam': 524, b'dam': 3168}\n",
      "validation_log {b'not_a_dam': 143, b'dam': 763}\n",
      "test_log {b'not_a_dam': 154, b'dam': 946}\n",
      "southaf_log {b'not_a_dam': 17, b'dam': 85}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 29\n",
      "training_log {b'not_a_dam': 540, b'dam': 3290}\n",
      "validation_log {b'not_a_dam': 148, b'dam': 787}\n",
      "test_log {b'not_a_dam': 156, b'dam': 974}\n",
      "southaf_log {b'not_a_dam': 17, b'dam': 88}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 30\n",
      "training_log {b'not_a_dam': 560, b'dam': 3395}\n",
      "validation_log {b'not_a_dam': 152, b'dam': 811}\n",
      "test_log {b'not_a_dam': 162, b'dam': 1013}\n",
      "southaf_log {b'not_a_dam': 18, b'dam': 89}\n",
      "\n",
      "\n",
      "Starting to make TFRecords 31\n"
     ]
    }
   ],
   "source": [
    "# Make workspace outputs directories doesn't exist\n",
    "directories_to_make = [WORKSPACE_DIR,\n",
    "                       os.path.join(WORKSPACE_DIR,'training_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'validation_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'test_set'),\n",
    "                       os.path.join(WORKSPACE_DIR,'southaf_set')]\n",
    "for directory in directories_to_make:\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "# Do the thing    \n",
    "training_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "validation_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "test_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "southaf_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "\n",
    "last_time = time.time()\n",
    "\n",
    "training_writer_count = 0\n",
    "validation_writer_count = 0\n",
    "tf_record_iteration = 0\n",
    "max_tf_record_iteration = int(len(all_images_file_list)/DAMS_PER_RECORD)\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    while tf_record_iteration <= max_tf_record_iteration:\n",
    "        \n",
    "        LOGGER.info('tf_record_iteration %d' % tf_record_iteration)\n",
    "\n",
    "        # Open writers\n",
    "        training_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'training_set/dams_%d.record' % tf_record_iteration))\n",
    "        validation_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'validation_set/dams_%d.record' % tf_record_iteration))\n",
    "        test_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                         'test_set/dams_%d.record' % tf_record_iteration))\n",
    "        southaf_writer = tf.python_io.TFRecordWriter(os.path.join(WORKSPACE_DIR,\n",
    "                     'southaf_set/dams_%d.record' % tf_record_iteration))\n",
    "        \n",
    "        # Get a slice of the dataset\n",
    "        slice_dam_images_list = all_images_file_list[tf_record_iteration*DAMS_PER_RECORD:min((tf_record_iteration+1)*DAMS_PER_RECORD,len(all_images_file_list))]\n",
    "\n",
    "        #This is where I make TFRecords!\n",
    "        training_log, validation_log, test_log, southaf_log = make_TFRecords(slice_dam_images_list,tf_record_iteration)\n",
    "        \n",
    "        # Close writers\n",
    "        training_writer.close()\n",
    "        validation_writer.close()\n",
    "        southaf_writer.close()\n",
    "        test_writer.close()\n",
    "        \n",
    "        # Advance loop\n",
    "        tf_record_iteration += 1\n",
    "\n",
    "        print('training_log',training_log)\n",
    "        print('validation_log',validation_log)\n",
    "        print('test_log',test_log)\n",
    "        print('southaf_log',southaf_log)\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "    \n",
    "#@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel available now\n"
     ]
    }
   ],
   "source": [
    "print('Kernel available now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Started around 14h30. Env 205 tf_iterations to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes & stuffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative to shapefiles of South Africa - harcoding lat/lon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_TFRecords_withSouthAf_rectangle(dam_file_list,tf_record_iteration):\n",
    "    '''Function to make_TFRecords from a list of images_paths'''\n",
    "    \n",
    "    print('Starting to make TFRecords %d' % tf_record_iteration)\n",
    "    \n",
    "\n",
    "    for image_path in dam_file_list:\n",
    "#             current_time = time.time()\n",
    "#             if current_time - last_time > 5.0:\n",
    "#                 LOGGER.info('training_log: %s', training_log)\n",
    "#                 LOGGER.info('validation_log: %s', validation_log)\n",
    "#                 LOGGER.info('southaf_log: %s', southaf_log)\n",
    "#                 LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "#                 LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "#                 last_time = current_time\n",
    "\n",
    "        #  Note from Rich:\n",
    "        # looks like anything can be used here, including serializing\n",
    "        # a tensor tf.serialize_tensor\n",
    "        image_string = tf.read_file(image_path)\n",
    "        image_decoded = tf.image.decode_png(image_string).eval()\n",
    "        image_string = open(image_path, 'rb').read()\n",
    "        feature_dict = {\n",
    "            'image/height': int64_feature(\n",
    "                image_decoded.shape[0]),\n",
    "            'image/width': int64_feature(\n",
    "                image_decoded.shape[1]),\n",
    "            'image/filename': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/source_id': bytes_feature(\n",
    "                bytes(image_path, 'utf8')),\n",
    "            'image/encoded': bytes_feature(image_string),\n",
    "            'image/format': bytes_feature(b'png'),\n",
    "        }\n",
    "\n",
    "        # if this image is a dam:\n",
    "\n",
    "        json_path = image_path.replace('.png', '.json')\n",
    "        if os.path.exists(json_path):\n",
    "            dam_type = b'dam'\n",
    "\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "            xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "            ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "            ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "            if (xmin < 0 or ymin < 0 or xmax >= 1 or ymax >= 1):\n",
    "                LOGGER.warning('bounding box out of bounds %s %s %s %s',\n",
    "                               xmin, xmax, ymin, ymax)\n",
    "                xmin = max(0, xmin)\n",
    "                xmax = min(xmax, 1)\n",
    "                ymin = max(0, ymin)\n",
    "                ymax = min(ymax, 1)\n",
    "\n",
    "            feature_dict.update({\n",
    "                'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                'image/object/class/label': int64_list_feature(\n",
    "                    [1]),  # the '1' is type 1 which is a dam\n",
    "                'image/object/class/text': bytes_list_feature(\n",
    "                    [b'dam']),\n",
    "            })\n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "        else:\n",
    "            dam_type = b'not_a_dam'\n",
    "            \n",
    "            json_path = image_path.replace('_bb.png', '.json')\n",
    "            with open(json_path, 'r') as json_file:\n",
    "                image_metadata = json.load(json_file)\n",
    "            \n",
    "            tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                feature=feature_dict))\n",
    "\n",
    "        \n",
    "        # Choose whether this record will go to training or validation (=dev) set \n",
    "\n",
    "        # For now only have centroids for dams (no lat/lon for not_a_dams, so fixinf to 99999 NoData for now) !!\n",
    "        try:\n",
    "            centroid = image_metadata['lng_lat_centroid']\n",
    "        except NameError:\n",
    "            raise Exception(\"Missing lat/lon for in file\", json_path)\n",
    "        \n",
    "        # WHEN GDAL WORKS:\n",
    "        #centroid = image_metadata['lng_lat_centroid']\n",
    "        #if sa_geom_prep.contains(shapely.geometry.Point(centroid[0], centroid[1])): # removed: and dam_type == b'dam'\n",
    "        # Till then, hard code:\n",
    "\n",
    "\n",
    "        #South Africa coordinates\n",
    "        southaf_max_lon = -26.958585\n",
    "        southaf_max_lat = 29.682244\n",
    "        southaf_min_lon = -28.756566\n",
    "        southaf_min_lat = 19.959007\n",
    "\n",
    "        if ((southaf_min_lon < centroid[0] < southaf_max_lon) and\n",
    "            (southaf_min_lat < centroid[1] < southaf_max_lat)):\n",
    "            writer = southaf_writer\n",
    "            log = southaf_log\n",
    "        elif numpy.random.random() > dev_set_portion:\n",
    "            writer = training_writer\n",
    "            log = training_log\n",
    "        #elif POSSIBLY ADD MORE TEST SET HERE?\n",
    "        #    writer = test_writer\n",
    "        #    log = test_log\n",
    "        else:\n",
    "            writer = validation_writer\n",
    "            log = validation_log\n",
    "        writer.write(tf_record.SerializeToString())\n",
    "\n",
    "        # Add stats \n",
    "        log[dam_type] += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return training_log, validation_log, test_log, southaf_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rich's orginial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Entry point.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(WORKSPACE_DIR)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    training_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "    validation_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "    south_africa_log = {b'dam': 0, b'not_a_dam': 0}\n",
    "\n",
    "    last_time = time.time()\n",
    "    training_writer_count = 0\n",
    "    validation_writer_count = 0\n",
    "    tf_record_iteration = 0\n",
    "\n",
    "    ## Subsets for Dev\n",
    "    dam_file_iter = glob.iglob(os.path.join(DAM_IMAGE_DIR, '119*pped.png'))\n",
    "    no_a_dam_file_iter = glob.iglob(os.path.join(NOT_A_DAM_IMAGE_DIR, '350*.png'))\n",
    "    \n",
    "    ## REAL DATASET FULL\n",
    "    # dam_file_iter = glob.iglob(os.path.join(DAM_IMAGE_DIR, '*pped.png'))\n",
    "    # no_a_dam_file_iter = glob.iglob(os.path.join(NOT_A_DAM_IMAGE_DIR, '*.png'))\n",
    "    \n",
    "    #  NOT USING THIS BELOW BECAUSE NOT DOING 50/50.\n",
    "    #dam_list_iter = [\n",
    "    #    path for path_tuple in zip(dam_file_iter, no_a_dam_file_iter)\n",
    "    #    for path in path_tuple]\n",
    "\n",
    "    #while True:\n",
    "    #    # this makes DAMS_PER_RECORD list of files\n",
    "    #    dam_file_list = [\n",
    "    #        (path, dam_type) for path, dam_type in zip(\n",
    "    #            itertools.islice(\n",
    "    #                dam_list_iter,\n",
    "    #                DAMS_PER_RECORD*tf_record_iteration,\n",
    "    #                DAMS_PER_RECORD*(tf_record_iteration+1)),\n",
    "    #            itertools.cycle([b'dam', b'not_a_dam']))]\n",
    "\n",
    "        LOGGER.debug(dam_file_list)\n",
    "        if not dam_file_list:\n",
    "            break\n",
    "\n",
    "        with tf.Graph().as_default(), tf.Session() as sess:\n",
    "            training_writer = tf.python_io.TFRecordWriter(\n",
    "                os.path.join(\n",
    "                    WORKSPACE_DIR,\n",
    "                    'dam_training_%d.record' % tf_record_iteration))\n",
    "            validation_writer = tf.python_io.TFRecordWriter(\n",
    "                os.path.join(\n",
    "                    WORKSPACE_DIR,\n",
    "                    'dam_validation_%d.record' % tf_record_iteration))\n",
    "            south_africa_writer = tf.python_io.TFRecordWriter(os.path.join(\n",
    "                WORKSPACE_DIR, 'south_africa_%d.record' %\n",
    "                tf_record_iteration))\n",
    "\n",
    "            for image_path, dam_type in dam_file_list:\n",
    "                current_time = time.time()\n",
    "                if current_time - last_time > 5.0:\n",
    "                    LOGGER.info('training_log: %s', training_log)\n",
    "                    LOGGER.info('validation_log: %s', validation_log)\n",
    "                    LOGGER.info('south_africa_log: %s', south_africa_log)\n",
    "                    LOGGER.info('training_writer_count: %d', training_writer_count)\n",
    "                    LOGGER.info('validation_writer_count: %d', validation_writer_count)\n",
    "                    last_time = current_time\n",
    "                # looks like anything can be used here, including serializing\n",
    "                # a tensor tf.serialize_tensor\n",
    "                image_string = tf.read_file(image_path)\n",
    "                image_decoded = tf.image.decode_png(image_string).eval()\n",
    "                image_string = open(image_path, 'rb').read()\n",
    "                feature_dict = {\n",
    "                    'image/height': int64_feature(\n",
    "                        image_decoded.shape[0]),\n",
    "                    'image/width': int64_feature(\n",
    "                        image_decoded.shape[1]),\n",
    "                    'image/filename': bytes_feature(\n",
    "                        bytes(image_path, 'utf8')),\n",
    "                    'image/source_id': bytes_feature(\n",
    "                        bytes(image_path, 'utf8')),\n",
    "                    'image/encoded': bytes_feature(image_string),\n",
    "                    'image/format': bytes_feature(b'png'),\n",
    "                }\n",
    "                if dam_type == b'dam':\n",
    "                    json_path = image_path.replace('.png', '.json')\n",
    "                    with open(json_path, 'r') as json_file:\n",
    "                        image_metadata = json.load(json_file)\n",
    "                    xmin = image_metadata['pixel_bounding_box'][0] / float(image_decoded.shape[0])\n",
    "                    xmax = image_metadata['pixel_bounding_box'][2] / float(image_decoded.shape[0])\n",
    "                    ymin = image_metadata['pixel_bounding_box'][3] / float(image_decoded.shape[1])\n",
    "                    ymax = image_metadata['pixel_bounding_box'][1] / float(image_decoded.shape[1])\n",
    "                    if (xmin < 0 or ymin < 0 or\n",
    "                            xmax >= 1 or\n",
    "                            ymax >= 1):\n",
    "                        LOGGER.warning(\n",
    "                            'bounding box out of bounds %s %s %s %s',\n",
    "                            xmin, xmax, ymin, ymax)\n",
    "                        xmin = max(0, xmin)\n",
    "                        xmax = min(xmax, 1)\n",
    "                        ymin = max(0, ymin)\n",
    "                        ymax = min(ymax, 1)\n",
    "\n",
    "                    feature_dict.update({\n",
    "                        'image/object/bbox/xmin': float_list_feature([xmin]),\n",
    "                        'image/object/bbox/xmax': float_list_feature([xmax]),\n",
    "                        'image/object/bbox/ymin': float_list_feature([ymin]),\n",
    "                        'image/object/bbox/ymax': float_list_feature([ymax]),\n",
    "                        'image/object/class/label': int64_list_feature(\n",
    "                            [1]),  # the '1' is type 1 which is a dam\n",
    "                        'image/object/class/text': bytes_list_feature(\n",
    "                            [b'dam']),\n",
    "                    })\n",
    "                    tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                        feature=feature_dict))\n",
    "\n",
    "                    centroid = image_metadata['lng_lat_centroid']\n",
    "                    if dam_type == b'dam' and sa_geom_prep.contains(\n",
    "                            shapely.geometry.Point(centroid[0], centroid[1])):\n",
    "                        writer = south_africa_writer\n",
    "                        log = south_africa_log\n",
    "                        writer.write(tf_record.SerializeToString())\n",
    "                        log[dam_type] += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    tf_record = tf.train.Example(features=tf.train.Features(\n",
    "                        feature=feature_dict))\n",
    "                if numpy.random.random() > dev_set_portion:\n",
    "                    writer = training_writer\n",
    "                    log = training_log\n",
    "                else:\n",
    "                    writer = validation_writer\n",
    "                    log = validation_log\n",
    "                writer.write(tf_record.SerializeToString())\n",
    "                log[dam_type] += 1\n",
    "\n",
    "            LOGGER.info(\n",
    "                \"training writer full creating %d instance\" %\n",
    "                tf_record_iteration)\n",
    "            tf_record_iteration += 1\n",
    "            training_writer.close()\n",
    "            validation_writer.close()\n",
    "            south_africa_writer.close()\n",
    "\n",
    "    with open('write_stats.txt', 'w') as write_stats_file:\n",
    "        write_stats_file.write(\n",
    "            f\"\"\"validation: dam({validation_log[b'dam']}) not_a_dam({\n",
    "                validation_log[b'not_a_dam']})\\n\"\"\"\n",
    "            f\"\"\"training: dam({training_log[b'dam']}) not_a_dam({\n",
    "                training_log[b'not_a_dam']})\\n\"\"\"\n",
    "            f\"\"\"south_africa: dam({south_africa_log[b'dam']}) not_a_dam({\n",
    "                south_africa_log[b'not_a_dam']})\\n\"\"\")\n",
    "\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
